{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "import io\n",
    "import pprint as pp\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeleniumClient(object):\n",
    "    def __init__(self):\n",
    "        #Initialization method. \n",
    "        self.chrome_options = webdriver.ChromeOptions()\n",
    "        self.chrome_options.add_argument('--headless')\n",
    "        self.chrome_options.add_argument('--no-sandbox')\n",
    "        self.chrome_options.add_argument('--disable-setuid-sandbox')\n",
    "\n",
    "        # you need to provide the path of chromdriver in your system\n",
    "        self.browser = webdriver.Chrome('/Users/aakashvardhan/Downloads/chromedriver', options=self.chrome_options)\n",
    "\n",
    "        self.base_url = 'https://twitter.com/search?q='\n",
    "\n",
    "    def search_twitter(self,query):\n",
    "        self.browser.get(self.base_url+query)\n",
    "        time.sleep(2)\n",
    " \n",
    "        # wait until the search box has loaded:\n",
    "        #box = driver.wait.until(EC.presence_of_element_located((By.NAME, \"q\")))\n",
    "\n",
    "        # find the search box in the html:\n",
    "        box=self.browser.find_element_by_xpath(\"//input[@placeholder='Search Twitter']\")\n",
    "        box.click()\n",
    "        # enter your search string in the search box:\n",
    "        box.send_keys(query)\n",
    "\n",
    "        # submit the query (like hitting return):\n",
    "        box.submit()\n",
    "\n",
    "        # initial wait for the search results to load\n",
    "        self.browser.implicitly_wait(5)\n",
    "        url=self.browser.current_url\n",
    "        return url\n",
    "\n",
    "    #url =\"https://twitter.com/search?q=pollution%20exclude%3Aretweets&src=typed_query\"  #eg: https://www.twitter.com/xyz/\n",
    "\n",
    "    #this function is to handle dynamic page content loading - using Selenium\n",
    "    def tweet_scroller(self,url):\n",
    "\n",
    "        browser.get(url)\n",
    "\n",
    "        #define initial page height for 'while' loop\n",
    "        lastHeight = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "        while True:\n",
    "            browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "            #define how many seconds to wait while dynamic page content loads\n",
    "            time.sleep(3)\n",
    "            newHeight = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "            if newHeight == lastHeight:\n",
    "                break\n",
    "            else:\n",
    "                lastHeight = newHeight\n",
    "\n",
    "        html = browser.page_source\n",
    "\n",
    "        return html\n",
    "\n",
    "\n",
    "\n",
    "    #function to handle/parse HTML and extract data - using BeautifulSoup    \n",
    "    def scrapper(self,url):\n",
    "\n",
    "        #regex patterns\n",
    "        url_finder = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        problemchars = re.compile(r'[\\[=\\+/&<>;:!\\\\|*^\\'\"\\?%$@)(_\\,\\.\\t\\r\\n0-9-â€”\\]]')\n",
    "        prochar = '[(=\\-\\+\\:/&<>;|\\'\"\\?%#$@\\,\\._)]'\n",
    "        crp = re.compile(r'MoreCopy link to TweetEmbed Tweet|Reply')\n",
    "        retweet = re.compile(r\"(?<=Retweet:)(.*)(?=', u'R)\")\n",
    "        fave = re.compile(r\"(?<=Like:)(.*)(?=', u'Liked)\")\n",
    "        wrd = re.compile(r'[A-Z]+[a-z]*')\n",
    "        dgt = re.compile(r'\\d+')    \n",
    "\n",
    "\n",
    "        blog_list = []\n",
    "\n",
    "        #set to global in case you want to play around with the HTML later   \n",
    "        global soup    \n",
    "\n",
    "        #call dynamic page scroll function here\n",
    "        soup = BeautifulSoup(SeleniumClient.tweet_scroller(url), \"html.parser\")\n",
    "\n",
    "\n",
    "\n",
    "        for i in soup.find_all('li', {\"data-item-type\":\"tweet\"}):\n",
    "            user = (i.find('span', {'class':\"username js-action-profile-name\"}).get_text() if i.find('span', {'class':\"username js-action-profile-name\"}) is not None else \"\")\n",
    "            link = ('https://twitter.com' + i.small.a['href'] if i.small is not None else \"\")\n",
    "            date = (i.small.a['title'] if i.small is not None else \"\")\n",
    "            popular = (i.find('div', {'class': \"js-tweet-text-container\"}).get_text().replace('\\n','') if i.find('div', {'class': \"js-tweet-text-container\"}) is not None else \"\")\n",
    "            text = (i.p.get_text().replace('\\n','') if i.p is not None else \"\")\n",
    "            popular_text = [i + ':' + j  if len(dgt.findall(popular)) != 0 else '' for i, j in zip(wrd.findall(crp.sub('', popular)), dgt.findall(popular))]\n",
    "\n",
    "\n",
    "                #build dictionary to format data as key-pair value \n",
    "            blog_dict = {\n",
    "            \"header\": \"twitter_hashtag_\" + url.rsplit('/',2)[1],\n",
    "            \"url\": link,\n",
    "            \"user\": user,\n",
    "            \"date\": date,\n",
    "            \"popular\": popular_text,\n",
    "                #before text is stored URLs are removed - note: hash symbol is maintained to indicate hashtag term\n",
    "            \"blog_text\": problemchars.sub(' ', url_finder.sub('', text)),\n",
    "            \"like_fave\": (int(''.join(fave.findall(str(popular_text)))) if len(fave.findall(str(popular_text))) > 0 else ''),\n",
    "            \"share_retweet\": (int(''.join(retweet.findall(str(popular_text)))) if len(retweet.findall(str(popular_text))) > 0 else '')\n",
    "            }\n",
    "\n",
    "            blog_list.append(blog_dict)            \n",
    "\n",
    "\n",
    "        #call csv writer function and output file\n",
    "        writer_csv_3(blog_list)\n",
    "\n",
    "        return pp.pprint(blog_list[0:2])\n",
    "\n",
    "\n",
    "\n",
    "    #function to write CSV file\n",
    "    def writer_csv_3(self,blog_list):\n",
    "\n",
    "        #uses group name from URL to construct output file name\n",
    "        file_out = \"twitter_hashtag_{page}.csv\".format(page = url.rsplit('/',2)[1])\n",
    "\n",
    "        with io.open(file_out, \"w\", encoding=\"utf-8\") as csvfile:\n",
    "\n",
    "            writer = csv.writer(csvfile, lineterminator='\\n', delimiter=',', quotechar='\"')\n",
    "\n",
    "            for i in blog_list:\n",
    "                if len(i['blog_text']) > 0:\n",
    "                    newrow = i['header'], i['url'], i['user'], i['date'], i[\"popular\"],i[\"blog_text\"] ,i[\"like_fave\"], i[\"share_retweet\"]\n",
    "\n",
    "                    writer.writerow(newrow)                     \n",
    "                else:\n",
    "                    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tweet_scroller() missing 1 required positional argument: 'url'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-93aa7bc5cbb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtweets_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselenium_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_twitter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'drought exclude:retweets'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mselenium_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweets_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-21-902282b21f6e>\u001b[0m in \u001b[0;36mscrapper\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0;31m#call dynamic page scroll function here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0msoup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSeleniumClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtweet_scroller\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"html.parser\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tweet_scroller() missing 1 required positional argument: 'url'"
     ]
    }
   ],
   "source": [
    "selenium_client = SeleniumClient()\n",
    "\n",
    "tweets_df = selenium_client.search_twitter('drought exclude:retweets')\n",
    "selenium_client.scrapper(tweets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-env",
   "language": "python",
   "name": "research-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
