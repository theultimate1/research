{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all imports\n",
    "import json\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.cluster import KMeans\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_json(r'News_Category_Dataset_v2.json',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment=(data[\"category\"]==\"ENVIRONMENT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset=data[environment]\n",
    "dataset=dataset[\"headline\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer('english')\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z\\']+')\n",
    "#based on part of speech, the tokenize function would lemmatize the word\n",
    "spacy_obj=spacy.load('en',disable=['parser', 'ner'])\n",
    "def tokenize(text):\n",
    "    text=re.sub(\"^\\d+\\s|\\s\\d+\\s|\\s\\d+$\", \" \", text)\n",
    "    text = re.sub(\"\\d+\", \"\", text)\n",
    "    lemmatized=spacy_obj(text.lower())\n",
    "    return [token.lemma_ for token in lemmatized]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jayaram/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['make'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "punc = ['.', ',', '\"', \"'\", '?', '!', ':', ';', '(', ')', '[', ']',\"//\",\".org\",\"'s\", '{', '}',\"%\",\"$\",\"&\",\"+\",\"=\",\"-\",\"--\",\"-PRON-\",\"-pron-\",\"..\",\"...\",\"/\"]\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union(punc)\n",
    "vectorizer = TfidfVectorizer(stop_words = stop_words,tokenizer=tokenize,max_features=2000)\n",
    "vectorized=vectorizer.fit_transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "       n_clusters=23, n_init=10, n_jobs=-1, precompute_distances='auto',\n",
       "       random_state=None, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=vectorizer.get_feature_names()\n",
    "kmeans = KMeans(n_clusters = 23, n_init = 10, n_jobs = -1)\n",
    "kmeans.fit(vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : sound, metal, backyard, recording, hear, music, audio, cicada, scrap, temperature\n",
      "1 : climate, change, talk, week, need, say, study, ice, action, carbon\n",
      "2 : new, specie, york, farmer, discover, photo, energy, normal, animal, nyc\n",
      "3 : sea, rise, level, video, study, fall, photo, underwater, deep, melting\n",
      "4 : day, earth, mother, world, gift, photo, fall, green, celebrate, endanger\n",
      "5 : baby, photo, animal, week, panda, picture, zoo, elephant, gorilla, giraffes\n",
      "6 : animal, week, photo, picture, tiger, elephant, monkey, panda, lion, cub\n",
      "7 :  , use, park, photo, national, amazing, animal, odd, weather.com, way\n",
      "8 : time, celebrate, lapse, year, video, week, cool, wildlife, puppy, draw\n",
      "9 : keystone, xl, pipeline, sand, tar, obama, protester, arrest, student, department\n",
      "10 : extreme, weather, week, photo, hero, hill, hilarious, highlight, high, zoonose\n",
      "11 : photo, animal, life, nature, thing, pollution, world, big, storm, free\n",
      "12 : spill, bp, oil, trial, gulf, exxon, molasse, testify, halliburton, year\n",
      "13 : home, save, tip, money, orca, energy, rescue, winterize, food, abandon\n",
      "14 : public, kill, lion, preserve, transit, zoo, explorer, slaughter, cub, bear\n",
      "15 : project, reuse, diy, idea, old, shirt, t, craft, bag, make\n",
      "16 : green, eco, electric, friendly, tip, labor, burial, power, event, new\n",
      "17 : hurricane, sandy, storm, survivor, effort, u.s, east, flood, path, map\n",
      "18 : video, capture, make, camera, thing, pet, animal, puppy, kitten, panda\n",
      "19 : dog, cat, photo, video, pet, adorable, love, owner, rescue, head\n",
      "20 : say, water, study, food, global, warming, scientist, ban, new, highlight\n",
      "21 : oil, gas, california, future, natural, industry, fracke, plan, platform, drought\n",
      "22 : u.s, winter, fukushima, state, storm, hit, coast, american, outlook, hurricane\n"
     ]
    }
   ],
   "source": [
    "common_words = kmeans.cluster_centers_.argsort()[:,-1:-11:-1]\n",
    "for num, centroid in enumerate(common_words):\n",
    "    print(str(num) + ' : ' + ', '.join(words[word] for word in centroid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cluster 17,cluster 3,cluster 12,cluster 21,cluster 22, cluster 20. relevant water words and some of the irrelevant words with some connection to environment."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
