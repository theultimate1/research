{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figuring out the number of words in our dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import textblob\n",
    "import glob\n",
    "\n",
    "from six.moves import urllib\n",
    "\n",
    "import errno\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twitter Samples\n",
    "Not super useful as they are UK 2015 Election related."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "twitter_samples.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hopeless for tmr :(',\n",
       " \"Everything in the kids section of IKEA is so cute. Shame I'm nearly 19 in 2 months :(\",\n",
       " '@Hegelbon That heart sliding into the waste basket. :(',\n",
       " '“@ketchBurning: I hate Japanese call him \"bani\" :( :(”\\n\\nMe too',\n",
       " 'Dang starting next week I have \"work\" :(']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_samples.strings()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hopeless', 'for', 'tmr', ':(', 'Everything', 'in', 'the', 'kids', 'section', 'of']\n"
     ]
    }
   ],
   "source": [
    "tweets = twitter_samples.tokenized()\n",
    "tweet_dict = []\n",
    "for i in range(len(tweets)):\n",
    "    for j in range(len(tweets[i])):\n",
    "        tweet_dict.append(tweets[i][j])\n",
    "\n",
    "\n",
    "print(tweet_dict[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "print(len(twitter_samples.strings()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70175\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>coal ash water</td>\n",
       "      <td>1.140000e+18</td>\n",
       "      <td>*@noahcicero *@DaisyFried Coal ash ponds are t...</td>\n",
       "      <td>6/26/2019 3:22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coal ash water</td>\n",
       "      <td>1.140000e+18</td>\n",
       "      <td>\"NC wants to find the source of water contamin...</td>\n",
       "      <td>6/25/2019 15:30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>coal ash water</td>\n",
       "      <td>1.140000e+18</td>\n",
       "      <td>*@atrupar Making great progress?\\n you're doin...</td>\n",
       "      <td>6/25/2019 14:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>coal ash water</td>\n",
       "      <td>1.140000e+18</td>\n",
       "      <td>As if we needed more reasons to ditch coal, it...</td>\n",
       "      <td>6/25/2019 9:36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>coal ash water</td>\n",
       "      <td>1.140000e+18</td>\n",
       "      <td>*@shossy2 *@RodneyClaeys W/ Obama's restrictio...</td>\n",
       "      <td>6/25/2019 0:42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         category      tweet_id  \\\n",
       "0  coal ash water  1.140000e+18   \n",
       "1  coal ash water  1.140000e+18   \n",
       "2  coal ash water  1.140000e+18   \n",
       "3  coal ash water  1.140000e+18   \n",
       "4  coal ash water  1.140000e+18   \n",
       "\n",
       "                                                text             date  \n",
       "0  *@noahcicero *@DaisyFried Coal ash ponds are t...   6/26/2019 3:22  \n",
       "1  \"NC wants to find the source of water contamin...  6/25/2019 15:30  \n",
       "2  *@atrupar Making great progress?\\n you're doin...  6/25/2019 14:43  \n",
       "3  As if we needed more reasons to ditch coal, it...   6/25/2019 9:36  \n",
       "4  *@shossy2 *@RodneyClaeys W/ Obama's restrictio...   6/25/2019 0:42  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scraped_files = glob.glob('./datasets/water_tweets/gathered_water/*.csv')\n",
    "scraped_tweets = pd.concat([pd.read_csv(f, encoding = 'ISO-8859-1', delimiter = ';') for f in scraped_files])\n",
    "scraped_tweets = scraped_tweets[scraped_tweets['text'] != 'text']\n",
    "scraped_tweets = scraped_tweets.drop_duplicates(subset = ['text'], keep = 'first').reset_index()\n",
    "scraped_tweets = scraped_tweets[['category', 'tweet_id', 'text', 'date']]\n",
    "print(len(scraped_tweets))\n",
    "scraped_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83099\n",
      "Index(['category', 'tweet_id', 'text', 'date'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>1.152000e+18</td>\n",
       "      <td>If you do the Jeep wave Iâm cutting your fin...</td>\n",
       "      <td>7/18/2019 23:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the</td>\n",
       "      <td>1.152000e+18</td>\n",
       "      <td>*@pokelover941 *@PoGoMaster5000 *@Kelven91 To ...</td>\n",
       "      <td>7/18/2019 23:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the</td>\n",
       "      <td>1.152000e+18</td>\n",
       "      <td>I actually went to the gym during the day who ...</td>\n",
       "      <td>7/18/2019 23:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the</td>\n",
       "      <td>1.152000e+18</td>\n",
       "      <td>*@PralineQueen1 I am quite looking forward to ...</td>\n",
       "      <td>7/18/2019 23:38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>the</td>\n",
       "      <td>1.152000e+18</td>\n",
       "      <td>*@CoryBooker Is it possible said officer was l...</td>\n",
       "      <td>7/18/2019 23:38</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  category      tweet_id                                               text  \\\n",
       "0      the  1.152000e+18  If you do the Jeep wave Iâm cutting your fin...   \n",
       "1      the  1.152000e+18  *@pokelover941 *@PoGoMaster5000 *@Kelven91 To ...   \n",
       "2      the  1.152000e+18  I actually went to the gym during the day who ...   \n",
       "3      the  1.152000e+18  *@PralineQueen1 I am quite looking forward to ...   \n",
       "4      the  1.152000e+18  *@CoryBooker Is it possible said officer was l...   \n",
       "\n",
       "              date  \n",
       "0  7/18/2019 23:38  \n",
       "1  7/18/2019 23:38  \n",
       "2  7/18/2019 23:38  \n",
       "3  7/18/2019 23:38  \n",
       "4  7/18/2019 23:38  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unrel_files = glob.glob('./datasets/water_tweets/rnd_tweets/*.csv')\n",
    "unrel = pd.concat([pd.read_csv(f, encoding = 'ISO-8859-1', delimiter = ';') for f in unrel_files])\n",
    "unrel = unrel[unrel['text'] != 'text']\n",
    "unrel = unrel.drop_duplicates(subset = ['text'], keep = 'first').reset_index()\n",
    "print(len(unrel))\n",
    "unrel = unrel.iloc[:, :5]\n",
    "unrel = unrel.iloc[:, 1:]\n",
    "unrel.to_csv(r'datasets\\water_tweets\\unrelated_tweets.csv', sep = ';')\n",
    "print(unrel.columns)\n",
    "unrel.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83099\n"
     ]
    }
   ],
   "source": [
    "unrel_text = np.array(unrel.text.astype(str))\n",
    "\n",
    "from textblob import TextBlob\n",
    "unrel_list = []\n",
    "for t in range(len(unrel_text)):\n",
    "    new = TextBlob(unrel_text[t])\n",
    "    unrel_list.append(new)\n",
    "\n",
    "unrel_words = []\n",
    "for i in range(len(unrel_list)):\n",
    "    tweet_i = unrel_list[i]\n",
    "    new_words = tweet_i.words\n",
    "    unrel_words.append(new_words)\n",
    "    \n",
    "print(len(unrel_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "unrel_words = np.array(list(chain(*unrel_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\debro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['*@noahcicero *@DaisyFried Coal ash ponds are the demonic mockery of water, but yeah, even logs can thrive and yearn in any floating hell...',\n",
       " '\"NC wants to find the source of water contaminants near the Chatham coal ash landfill\" #ncpol https://t.co/cAQN72okYW',\n",
       " \"*@atrupar Making great progress?\\n you're doing everything you can to\\n Reverse water/air Standards\\nThe Trump administration just revoked the rule that prevented coal companies from  dumping coal Ash in Americas streams\\n\\nAlong with\\nMANY OTHERS\\n\\nhttps://t.co/VWiDvkWr8r\",\n",
       " 'As if we needed more reasons to ditch coal, it\\'s also #radioactive. \"ounce for ounce, coal ash released from a power plant delivers more #radiation than nuclear waste shielded via water or dry cask storage.\" #nocleancoal \\nhttps://t.co/LZgh5PlHZx',\n",
       " '*@shossy2 *@RodneyClaeys W/ Obama\\'s restrictions Duke Energy got away with \"environmental murder\" in NC.Coal ash pond emptied via pipe directly into Dan River.*@VP *@POTUS *@GOP,&amp  *@EPA,you ALL work for US and most of us care about our land, water, and air.*@VP,how about those of us with respiratory diseases?']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "water_text = np.array(scraped_tweets.text.astype(str))\n",
    "\n",
    "water_list = []\n",
    "for i in range(len(water_text)):\n",
    "    tweet = water_text[i]\n",
    "    water_list.append(tweet)\n",
    "water_list[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TextBlob(\"*@noahcicero *@DaisyFried Coal ash ponds are the demonic mockery of water, but yeah, even logs can thrive and yearn in any floating hell...\"), TextBlob(\"\"NC wants to find the source of water contaminants near the Chatham coal ash landfill\" #ncpol https://t.co/cAQN72okYW\"), TextBlob(\"*@atrupar Making great progress?\n",
      " you're doing everything you can to\n",
      " Reverse water/air Standards\n",
      "The Trump administration just revoked the rule that prevented coal companies from  dumping coal Ash in Americas streams\n",
      "\n",
      "Along with\n",
      "MANY OTHERS\n",
      "\n",
      "https://t.co/VWiDvkWr8r\"), TextBlob(\"As if we needed more reasons to ditch coal, it's also #radioactive. \"ounce for ounce, coal ash released from a power plant delivers more #radiation than nuclear waste shielded via water or dry cask storage.\" #nocleancoal \n",
      "https://t.co/LZgh5PlHZx\"), TextBlob(\"*@shossy2 *@RodneyClaeys W/ Obama's restrictions Duke Energy got away with \"environmental murder\" in NC.Coal ash pond emptied via pipe directly into Dan River.*@VP *@POTUS *@GOP,&amp  *@EPA,you ALL work for US and most of us care about our land, water, and air.*@VP,how about those of us with respiratory diseases?\")]\n"
     ]
    }
   ],
   "source": [
    "water = []\n",
    "for i in range(len(water_list)):\n",
    "    tweet = str(water_list[i])\n",
    "    new_tweet = TextBlob(tweet)\n",
    "    water.append(new_tweet)\n",
    "    \n",
    "print(water[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WordList(['noahcicero', 'DaisyFried', 'Coal', 'ash', 'ponds', 'are', 'the', 'demonic', 'mockery', 'of', 'water', 'but', 'yeah', 'even', 'logs', 'can', 'thrive', 'and', 'yearn', 'in', 'any', 'floating', 'hell']), WordList(['NC', 'wants', 'to', 'find', 'the', 'source', 'of', 'water', 'contaminants', 'near', 'the', 'Chatham', 'coal', 'ash', 'landfill', 'ncpol', 'https', 't.co/cAQN72okYW'])]\n"
     ]
    }
   ],
   "source": [
    "rel_h2o_words = []\n",
    "for i in range(len(water)):\n",
    "    tweet_i = water[i]\n",
    "    new_words = tweet_i.words\n",
    "    rel_h2o_words.append(new_words)\n",
    "    \n",
    "print(rel_h2o_words[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_words = np.array(tweet_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "rel_h2o_words = np.array(list(chain(*rel_h2o_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4623892\n"
     ]
    }
   ],
   "source": [
    "print(len(tweet_words) + len(rel_h2o_words) + len(unrel_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete = []\n",
    "tweet_words = list(tweet_words)\n",
    "rel_h2o_words = list(rel_h2o_words)\n",
    "unrel_words = list(unrel_words)\n",
    "#complete = list(complete.append([tweet_words, rel_h2o_words, unrel_words]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-8db38c912554>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcomplete\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtweet_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrel_h2o_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munrel_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0munique\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomplete\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomplete\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "complete = np.concatenate([tweet_words, rel_h2o_words, unrel_words])\n",
    "unique = np.unique(complete)\n",
    "print(len(unique), len(complete))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
