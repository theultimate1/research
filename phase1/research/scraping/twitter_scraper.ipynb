{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "import io\n",
    "import pprint as pp\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "def login_twitter(driver, username, password):\n",
    " \n",
    "    # open the web page in the browser:\n",
    "    driver.get(\"https://twitter.com/login\")\n",
    " \n",
    "    # find the boxes for username and password\n",
    "    username_field = driver.find_element_by_class_name(\"js-username-field\")\n",
    "    password_field = driver.find_element_by_class_name(\"js-password-field\")\n",
    " \n",
    "    # enter your username:\n",
    "    username_field.send_keys(username)\n",
    "    driver.implicitly_wait(1)\n",
    " \n",
    "    # enter your password:\n",
    "    password_field.send_keys(password)\n",
    "    driver.implicitly_wait(1)\n",
    " \n",
    "    # click the \"Log In\" button:\n",
    "    driver.find_element_by_class_name(\"EdgeButtom--medium\").click()\n",
    "    \n",
    "    return\n",
    "def search_twitter(driver, query):\n",
    " \n",
    "    # wait until the search box has loaded:\n",
    "    #box = driver.wait.until(EC.presence_of_element_located((By.NAME, \"q\")))\n",
    " \n",
    "    # find the search box in the html:\n",
    "    box=driver.find_element_by_xpath(\"//input[@placeholder='Search Twitter']\")\n",
    "    box.click()\n",
    "    # enter your search string in the search box:\n",
    "    box.send_keys(query)\n",
    " \n",
    "    # submit the query (like hitting return):\n",
    "    box.submit()\n",
    " \n",
    "    # initial wait for the search results to load\n",
    "    driver.implicitly_wait(5)\n",
    "    url=driver.current_url\n",
    "    return url\n",
    "\n",
    "#url =\"https://twitter.com/search?q=pollution%20exclude%3Aretweets&src=typed_query\"  #eg: https://www.twitter.com/xyz/\n",
    "\n",
    "#this function is to handle dynamic page content loading - using Selenium\n",
    "def tweet_scroller(url):\n",
    "\n",
    "    browser.get(url)\n",
    "    \n",
    "    #define initial page height for 'while' loop\n",
    "    lastHeight = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "    \n",
    "    while True:\n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "        #define how many seconds to wait while dynamic page content loads\n",
    "        time.sleep(3)\n",
    "        newHeight = browser.execute_script(\"return document.body.scrollHeight\")\n",
    "        \n",
    "        if newHeight == lastHeight:\n",
    "            break\n",
    "        else:\n",
    "            lastHeight = newHeight\n",
    "            \n",
    "    html = browser.page_source\n",
    "\n",
    "    return html\n",
    "\n",
    "\n",
    "    \n",
    "#function to handle/parse HTML and extract data - using BeautifulSoup    \n",
    "def scrapper(url):\n",
    "    \n",
    "    #regex patterns\n",
    "    url_finder = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    problemchars = re.compile(r'[\\[=\\+/&<>;:!\\\\|*^\\'\"\\?%$@)(_\\,\\.\\t\\r\\n0-9-â€”\\]]')\n",
    "    prochar = '[(=\\-\\+\\:/&<>;|\\'\"\\?%#$@\\,\\._)]'\n",
    "    crp = re.compile(r'MoreCopy link to TweetEmbed Tweet|Reply')\n",
    "    retweet = re.compile(r\"(?<=Retweet:)(.*)(?=', u'R)\")\n",
    "    fave = re.compile(r\"(?<=Like:)(.*)(?=', u'Liked)\")\n",
    "    wrd = re.compile(r'[A-Z]+[a-z]*')\n",
    "    dgt = re.compile(r'\\d+')    \n",
    "    \n",
    "\n",
    "    blog_list = []\n",
    "     \n",
    "    #set to global in case you want to play around with the HTML later   \n",
    "    global soup    \n",
    "    \n",
    "    #call dynamic page scroll function here\n",
    "    soup = BeautifulSoup(tweet_scroller(url), \"html.parser\")\n",
    "    \n",
    "    \n",
    "        \n",
    "    for i in soup.find_all('li', {\"data-item-type\":\"tweet\"}):\n",
    "        user = (i.find('span', {'class':\"username js-action-profile-name\"}).get_text() if i.find('span', {'class':\"username js-action-profile-name\"}) is not None else \"\")\n",
    "        link = ('https://twitter.com' + i.small.a['href'] if i.small is not None else \"\")\n",
    "        date = (i.small.a['title'] if i.small is not None else \"\")\n",
    "        popular = (i.find('div', {'class': \"js-tweet-text-container\"}).get_text().replace('\\n','') if i.find('div', {'class': \"js-tweet-text-container\"}) is not None else \"\")\n",
    "        text = (i.p.get_text().replace('\\n','') if i.p is not None else \"\")\n",
    "        popular_text = [i + ':' + j  if len(dgt.findall(popular)) != 0 else '' for i, j in zip(wrd.findall(crp.sub('', popular)), dgt.findall(popular))]\n",
    "        \n",
    "            \n",
    "            #build dictionary to format data as key-pair value \n",
    "        blog_dict = {\n",
    "        \"header\": \"twitter_hashtag_\" + url.rsplit('/',2)[1],\n",
    "        \"url\": link,\n",
    "        \"user\": user,\n",
    "        \"date\": date,\n",
    "        \"popular\": popular_text,\n",
    "            #before text is stored URLs are removed - note: hash symbol is maintained to indicate hashtag term\n",
    "        \"blog_text\": problemchars.sub(' ', url_finder.sub('', text)),\n",
    "        \"like_fave\": (int(''.join(fave.findall(str(popular_text)))) if len(fave.findall(str(popular_text))) > 0 else ''),\n",
    "        \"share_retweet\": (int(''.join(retweet.findall(str(popular_text)))) if len(retweet.findall(str(popular_text))) > 0 else '')\n",
    "        }\n",
    "        \n",
    "        blog_list.append(blog_dict)            \n",
    "        \n",
    "            \n",
    "    #call csv writer function and output file\n",
    "    writer_csv_3(blog_list)\n",
    "    \n",
    "    return pp.pprint(blog_list[0:2])\n",
    "\n",
    "    \n",
    "    \n",
    "#function to write CSV file\n",
    "def writer_csv_3(blog_list):\n",
    "    \n",
    "    #uses group name from URL to construct output file name\n",
    "    file_out = \"twitter_hashtag_{page}.csv\".format(page = url.rsplit('/',2)[1])\n",
    "    \n",
    "    with io.open(file_out, \"w\", encoding=\"utf-8\") as csvfile:\n",
    "\n",
    "        writer = csv.writer(csvfile, lineterminator='\\n', delimiter=',', quotechar='\"')\n",
    "    \n",
    "        for i in blog_list:\n",
    "            if len(i['blog_text']) > 0:\n",
    "                newrow = i['header'], i['url'], i['user'], i['date'], i[\"popular\"],i[\"blog_text\"] ,i[\"like_fave\"], i[\"share_retweet\"]\n",
    "\n",
    "                writer.writerow(newrow)                     \n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    \n",
    "#main\n",
    "if __name__ == \"__main__\":\n",
    "    path_to_chromedriver =\"/Users/aakashvardhan/Downloads/chromedriver\"            #enter path of chromedriver\n",
    "    browser = webdriver.Chrome(executable_path = path_to_chromedriver)\n",
    "    query=\"water exclude:retweets\"\n",
    "    username = \"vardhan_aakash1\"\n",
    "    password = \"Galgaddot8763\"\n",
    "    login_twitter(browser, username, password)\n",
    "    url=search_twitter(browser,query)\n",
    "    scrapper(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research-env",
   "language": "python",
   "name": "research-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
