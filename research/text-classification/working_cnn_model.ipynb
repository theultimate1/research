{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Flatten, Dense, Dropout, Convolution1D, MaxPooling1D, SpatialDropout1D, Input \n",
    "from keras.layers import GlobalMaxPooling1D, concatenate, LSTM, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import scipy.stats as stats\n",
    "import string\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaner found on github: https://github.com/martinpella/twitter-airlines/blob/master/utils.py\n",
    "class TextCleaner(BaseEstimator, TransformerMixin):    \n",
    "    def remove_mentions(self, text):        \n",
    "        return re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    def remove_urls(self, text):        \n",
    "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', text)\n",
    "    \n",
    "    def only_characters(self, text):\n",
    "        return re.sub('[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    def remove_extra_spaces(self, text):\n",
    "        text = re.sub(\"\\s+\", ' ', text)\n",
    "        text = text.lstrip()\n",
    "        return text.rstrip()\n",
    "    \n",
    "    def to_lower(self, text):\n",
    "        return text.lower()\n",
    "    \n",
    "    def fix_words(self, text):\n",
    "        text = re.sub(r'\\bthx\\b', 'thanks', text)\n",
    "        text = re.sub(r'\\bu\\b', 'you', text)\n",
    "        text = re.sub(r'\\bhrs\\b', 'hours', text)\n",
    "        text = re.sub(r'\\baa\\b', 'a', text)\n",
    "        text = re.sub(r'\\bflightr\\b', 'flight', text)\n",
    "        text = re.sub(r'\\bur\\b', 'your', text)\n",
    "        text = re.sub(r'\\bhr\\b', 'hour', text)\n",
    "        text = re.sub(r'\\bthru\\b', 'through', text)\n",
    "        text = re.sub(r'\\br\\b', 'are', text)\n",
    "        text = re.sub(r'\\bppl\\b', 'people', text)\n",
    "        text = re.sub(r'\\btix\\b', 'fix', text)\n",
    "        text = re.sub(r'\\bplz\\b', 'please', text)\n",
    "        text = re.sub(r'\\bflightd\\b', 'flighted', text)\n",
    "        text = re.sub(r'\\btmrw\\b', 'tomorrow', text)\n",
    "        text = re.sub(r'\\bthx\\b', 'thanks', text)\n",
    "        text = re.sub(r'\\bpls\\b', 'please', text)\n",
    "        text = re.sub(r'\\bfyi\\b', 'for your information', text)\n",
    "        \n",
    "        text = re.sub(r'\\bheyyyy\\b', 'hey', text)\n",
    "        text = re.sub(r'\\bguyyyys\\b', 'guys', text)\n",
    "        text = re.sub(r'\\byall\\b', 'you all', text)\n",
    "        text = re.sub(r'\\basap\\b', 'as soon as possible', text)\n",
    "        text = re.sub(r'\\bbtw\\b', 'by the way', text)\n",
    "        text = re.sub(r'\\bdm\\b', 'direct message', text)\n",
    "        text = re.sub(r'\\bcudtomers\\b', 'customers', text)\n",
    "        text = re.sub(r'\\bwtf\\b', 'what the fuck', text)\n",
    "        text = re.sub(r'\\biphone\\b', 'phone', text)\n",
    "        text = re.sub(r'\\bmins\\b', 'minutes', text)\n",
    "        text = re.sub(r'\\btv\\b', 'television', text)\n",
    "        text = re.sub(r'\\bokay\\b', 'ok', text)\n",
    "        text = re.sub(r'\\bfeb\\b', 'february', text)\n",
    "        text = re.sub(r'\\byr\\b', 'year', text)\n",
    "        text = re.sub(r'\\bshes\\b', 'she is', text)\n",
    "        text = re.sub(r'\\bnope\\b', 'no', text)\n",
    "        text = re.sub(r'\\bhes\\b', 'he is', text)\n",
    "        text = re.sub(r'\\btill\\b', 'until', text)\n",
    "        text = re.sub(r'\\bomg\\b', 'oh my god', text)\n",
    "        text = re.sub(r'\\btho\\b', 'though', text)\n",
    "        text = re.sub(r'\\bnothappy\\b', 'not happy', text)\n",
    "        return re.sub(r'\\bthankyou\\b', 'thank you', text)\n",
    "        \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):        \n",
    "        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.only_characters).apply(self.remove_extra_spaces).apply(self.to_lower).apply(self.fix_words)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification</th>\n",
       "      <th>binary_class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun, Sand And Sewage: Report Shows Many U.S. B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Many U.S. Beaches Are Unsafe For Swimming, Rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Many U.S. Beaches Are Unsafe For Swimming, Rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun, Sand And Sewage: Report Shows Many U.S. B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun, Sand And Sewage: Report Shows Many U.S. B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  classification  binary_class  \\\n",
       "0       relevant             1   \n",
       "1       relevant             1   \n",
       "2       relevant             1   \n",
       "3       relevant             1   \n",
       "4       relevant             1   \n",
       "\n",
       "                                                text  \n",
       "0  Sun, Sand And Sewage: Report Shows Many U.S. B...  \n",
       "1  Many U.S. Beaches Are Unsafe For Swimming, Rep...  \n",
       "2  Many U.S. Beaches Are Unsafe For Swimming, Rep...  \n",
       "3  Sun, Sand And Sewage: Report Shows Many U.S. B...  \n",
       "4  Sun, Sand And Sewage: Report Shows Many U.S. B...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "relevant_tweets = pd.read_hdf('datasets/relevant_tweets.h5', 'relevant_tweets')\n",
    "relevant_tweets['classification'] = 'relevant'\n",
    "relevant_tweets['binary_class'] = np.ones(len(relevant_tweets)).astype(int)\n",
    "relevant_tweets = relevant_tweets[['classification', 'binary_class', 'text']]\n",
    "irrelevant_tweets = pd.read_hdf('datasets/not_relevant_tweets.h5', 'not_relevant_tweets')\n",
    "irrelevant_tweets['classification'] = 'irrelevant'\n",
    "irrelevant_tweets['binary_class'] = np.zeros(len(irrelevant_tweets)).astype(int)\n",
    "irrelevant_tweets = irrelevant_tweets[['classification', 'binary_class', 'text']]\n",
    "df = pd.concat([relevant_tweets, irrelevant_tweets]).reset_index()\n",
    "df = df.iloc[:, 1:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification</th>\n",
       "      <th>binary_class</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun, Sand And Sewage: Report Shows Many U.S. B...</td>\n",
       "      <td>sun sand and sewage report shows many us beach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Many U.S. Beaches Are Unsafe For Swimming, Rep...</td>\n",
       "      <td>many us beaches are unsafe for swimming report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Many U.S. Beaches Are Unsafe For Swimming, Rep...</td>\n",
       "      <td>many us beaches are unsafe for swimming report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun, Sand And Sewage: Report Shows Many U.S. B...</td>\n",
       "      <td>sun sand and sewage report shows many us beach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun, Sand And Sewage: Report Shows Many U.S. B...</td>\n",
       "      <td>sun sand and sewage report shows many us beach...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  classification  binary_class  \\\n",
       "0       relevant             1   \n",
       "1       relevant             1   \n",
       "2       relevant             1   \n",
       "3       relevant             1   \n",
       "4       relevant             1   \n",
       "\n",
       "                                                text  \\\n",
       "0  Sun, Sand And Sewage: Report Shows Many U.S. B...   \n",
       "1  Many U.S. Beaches Are Unsafe For Swimming, Rep...   \n",
       "2  Many U.S. Beaches Are Unsafe For Swimming, Rep...   \n",
       "3  Sun, Sand And Sewage: Report Shows Many U.S. B...   \n",
       "4  Sun, Sand And Sewage: Report Shows Many U.S. B...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  sun sand and sewage report shows many us beach...  \n",
       "1  many us beaches are unsafe for swimming report...  \n",
       "2  many us beaches are unsafe for swimming report...  \n",
       "3  sun sand and sewage report shows many us beach...  \n",
       "4  sun sand and sewage report shows many us beach...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the text\n",
    "tc = TextCleaner()\n",
    "df['cleaned_text'] = tc.transform(df['text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification</th>\n",
       "      <th>binary_class</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun, Sand And Sewage: Report Shows Many U.S. B...</td>\n",
       "      <td>sun sand and sewage report shows many us beach...</td>\n",
       "      <td>[sun, sand, and, sewage, report, shows, many, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Many U.S. Beaches Are Unsafe For Swimming, Rep...</td>\n",
       "      <td>many us beaches are unsafe for swimming report...</td>\n",
       "      <td>[many, us, beaches, are, unsafe, for, swimming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Many U.S. Beaches Are Unsafe For Swimming, Rep...</td>\n",
       "      <td>many us beaches are unsafe for swimming report...</td>\n",
       "      <td>[many, us, beaches, are, unsafe, for, swimming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun, Sand And Sewage: Report Shows Many U.S. B...</td>\n",
       "      <td>sun sand and sewage report shows many us beach...</td>\n",
       "      <td>[sun, sand, and, sewage, report, shows, many, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun, Sand And Sewage: Report Shows Many U.S. B...</td>\n",
       "      <td>sun sand and sewage report shows many us beach...</td>\n",
       "      <td>[sun, sand, and, sewage, report, shows, many, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  classification  binary_class  \\\n",
       "0       relevant             1   \n",
       "1       relevant             1   \n",
       "2       relevant             1   \n",
       "3       relevant             1   \n",
       "4       relevant             1   \n",
       "\n",
       "                                                text  \\\n",
       "0  Sun, Sand And Sewage: Report Shows Many U.S. B...   \n",
       "1  Many U.S. Beaches Are Unsafe For Swimming, Rep...   \n",
       "2  Many U.S. Beaches Are Unsafe For Swimming, Rep...   \n",
       "3  Sun, Sand And Sewage: Report Shows Many U.S. B...   \n",
       "4  Sun, Sand And Sewage: Report Shows Many U.S. B...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  sun sand and sewage report shows many us beach...   \n",
       "1  many us beaches are unsafe for swimming report...   \n",
       "2  many us beaches are unsafe for swimming report...   \n",
       "3  sun sand and sewage report shows many us beach...   \n",
       "4  sun sand and sewage report shows many us beach...   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [sun, sand, and, sewage, report, shows, many, ...  \n",
       "1  [many, us, beaches, are, unsafe, for, swimming...  \n",
       "2  [many, us, beaches, are, unsafe, for, swimming...  \n",
       "3  [sun, sand, and, sewage, report, shows, many, ...  \n",
       "4  [sun, sand, and, sewage, report, shows, many, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "\n",
    "def tokenize(s): \n",
    "    return re_tok.sub(r' \\1 ', s).split()\n",
    "\n",
    "df['tokenized'] = df['cleaned_text'].apply(lambda row: tokenize(row))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62688\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification</th>\n",
       "      <th>binary_class</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun, Sand And Sewage: Report Shows Many U.S. B...</td>\n",
       "      <td>sun sand and sewage report shows many us beach...</td>\n",
       "      <td>[sun, sand, and, sewage, report, shows, many, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Many U.S. Beaches Are Unsafe For Swimming, Rep...</td>\n",
       "      <td>many us beaches are unsafe for swimming report...</td>\n",
       "      <td>[many, us, beaches, are, unsafe, for, swimming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Many U.S. Beaches Are Unsafe For Swimming, Rep...</td>\n",
       "      <td>many us beaches are unsafe for swimming report...</td>\n",
       "      <td>[many, us, beaches, are, unsafe, for, swimming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun, Sand And Sewage: Report Shows Many U.S. B...</td>\n",
       "      <td>sun sand and sewage report shows many us beach...</td>\n",
       "      <td>[sun, sand, and, sewage, report, shows, many, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Thanks, EPA.\\n\\nSun, Sand And Sewage: Report S...</td>\n",
       "      <td>thanks epa sun sand and sewage report shows ma...</td>\n",
       "      <td>[thanks, epa, sun, sand, and, sewage, report, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  classification  binary_class  \\\n",
       "0       relevant             1   \n",
       "1       relevant             1   \n",
       "2       relevant             1   \n",
       "3       relevant             1   \n",
       "4       relevant             1   \n",
       "\n",
       "                                                text  \\\n",
       "0  Sun, Sand And Sewage: Report Shows Many U.S. B...   \n",
       "1  Many U.S. Beaches Are Unsafe For Swimming, Rep...   \n",
       "2  Many U.S. Beaches Are Unsafe For Swimming, Rep...   \n",
       "3  Sun, Sand And Sewage: Report Shows Many U.S. B...   \n",
       "4  Thanks, EPA.\\n\\nSun, Sand And Sewage: Report S...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  sun sand and sewage report shows many us beach...   \n",
       "1  many us beaches are unsafe for swimming report...   \n",
       "2  many us beaches are unsafe for swimming report...   \n",
       "3  sun sand and sewage report shows many us beach...   \n",
       "4  thanks epa sun sand and sewage report shows ma...   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [sun, sand, and, sewage, report, shows, many, ...  \n",
       "1  [many, us, beaches, are, unsafe, for, swimming...  \n",
       "2  [many, us, beaches, are, unsafe, for, swimming...  \n",
       "3  [sun, sand, and, sewage, report, shows, many, ...  \n",
       "4  [thanks, epa, sun, sand, and, sewage, report, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates(subset = ['cleaned_text']).reset_index()\n",
    "df = df.iloc[:, 1:]\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "stop = set(stopwords.words('english'))\n",
    "stop.update(['amp', 'rt', 'cc'])\n",
    "stop = stop - set(['no', 'not'])\n",
    "\n",
    "def remove_stopwords(row):\n",
    "    return [t for t in row if t not in stop]\n",
    "\n",
    "df['tokenized'] = df['tokenized'].apply(lambda row: remove_stopwords(row))\n",
    "\n",
    "df = df[['classification', 'binary_class', 'text', 'tokenized']]\n",
    "vocab_counter = collections.Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sun', 'sand']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = np.array(df['tokenized']).reshape(-1, 1)\n",
    "words = []\n",
    "for i in range(len(tokens)):\n",
    "    tweet = tokens[i]\n",
    "    for j in range(len(tweet)):\n",
    "        new_word = tweet[j]\n",
    "        words.append(new_word)\n",
    "\n",
    "print(type(words))\n",
    "indivs = []\n",
    "for tweet in range(len(words)):\n",
    "    t = words[tweet]\n",
    "    for j in range(len(t)):\n",
    "        indivs.append(t[j])\n",
    "indivs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 25000\n",
    "from collections import Counter\n",
    "\n",
    "# UNK = unknown words, HST = hashtag, EMT = emoticon, URL is self-explanatory, THDL = Twitter handle\n",
    "vocabulary = [(\"<UNK>\", None)] + Counter(indivs).most_common(vocabulary_size - 1)\n",
    "vocabulary = np.array([word for word, _ in vocabulary])\n",
    "dictionary = {word: code for code, word in enumerate(vocabulary)}\n",
    "data = np.array([dictionary.get(word, 0) for word in indivs])\n",
    "print(len(vocabulary))\n",
    "#print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 0 13.877185426237876\n"
     ]
    }
   ],
   "source": [
    "lengths = df['tokenized'].apply(lambda x: len(x))\n",
    "print(max(lengths), min(lengths), np.mean(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_id = {w:i for i, w in enumerate(vocabulary[:25000])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'<UNK>': 0,\n",
       " 'water': 1,\n",
       " 'not': 2,\n",
       " 'flood': 3,\n",
       " 'flash': 4,\n",
       " 'like': 5,\n",
       " 'no': 6,\n",
       " 'oil': 7,\n",
       " 'spill': 8,\n",
       " 'im': 9,\n",
       " 'people': 10,\n",
       " 'dont': 11,\n",
       " 'one': 12,\n",
       " 'warning': 13,\n",
       " 'get': 14,\n",
       " 'know': 15,\n",
       " 'time': 16,\n",
       " 'pm': 17,\n",
       " 'would': 18,\n",
       " 'contamination': 19,\n",
       " 'drought': 20,\n",
       " 'watch': 21,\n",
       " 'us': 22,\n",
       " 'see': 23,\n",
       " 'think': 24,\n",
       " 'good': 25,\n",
       " 'even': 26,\n",
       " 'still': 27,\n",
       " 'go': 28,\n",
       " 'well': 29,\n",
       " 'thats': 30,\n",
       " 'love': 31,\n",
       " 'want': 32,\n",
       " 'county': 33,\n",
       " 'much': 34,\n",
       " 'back': 35,\n",
       " 'could': 36,\n",
       " 'really': 37,\n",
       " 'also': 38,\n",
       " 'make': 39,\n",
       " 'de': 40,\n",
       " 'sanitation': 41,\n",
       " 'algae': 42,\n",
       " 'got': 43,\n",
       " 'drinking': 44,\n",
       " 'going': 45,\n",
       " 'way': 46,\n",
       " 'new': 47,\n",
       " 'cant': 48,\n",
       " 'right': 49,\n",
       " 'said': 50,\n",
       " 'never': 51,\n",
       " 'waste': 52,\n",
       " 'today': 53,\n",
       " 'bloom': 54,\n",
       " 'say': 55,\n",
       " 'take': 56,\n",
       " 'work': 57,\n",
       " 'day': 58,\n",
       " 'years': 59,\n",
       " 'first': 60,\n",
       " 'youre': 61,\n",
       " 'please': 62,\n",
       " 'need': 63,\n",
       " 'many': 64,\n",
       " 'via': 65,\n",
       " 'que': 66,\n",
       " 'didnt': 67,\n",
       " 'issued': 68,\n",
       " 'last': 69,\n",
       " 'look': 70,\n",
       " 'life': 71,\n",
       " 'sewage': 72,\n",
       " 'july': 73,\n",
       " 'rain': 74,\n",
       " 'clean': 75,\n",
       " 'help': 76,\n",
       " 'cdt': 77,\n",
       " 'trump': 78,\n",
       " 'great': 79,\n",
       " 'year': 80,\n",
       " 'la': 81,\n",
       " 'better': 82,\n",
       " 'use': 83,\n",
       " 'news': 84,\n",
       " 'every': 85,\n",
       " 'ive': 86,\n",
       " 'area': 87,\n",
       " 'always': 88,\n",
       " 'lake': 89,\n",
       " 'world': 90,\n",
       " 'man': 91,\n",
       " 'around': 92,\n",
       " 'come': 93,\n",
       " 'something': 94,\n",
       " 'made': 95,\n",
       " 'nws': 96,\n",
       " 'best': 97,\n",
       " 'flooding': 98,\n",
       " 'weather': 99,\n",
       " 'shit': 100,\n",
       " 'two': 101,\n",
       " 'may': 102,\n",
       " 'someone': 103,\n",
       " 'leak': 104,\n",
       " 'due': 105,\n",
       " 'doesnt': 106,\n",
       " 'bad': 107,\n",
       " 'oh': 108,\n",
       " 'sure': 109,\n",
       " 'thing': 110,\n",
       " 'food': 111,\n",
       " 'feel': 112,\n",
       " 'effect': 113,\n",
       " 'stop': 114,\n",
       " 'city': 115,\n",
       " 'things': 116,\n",
       " 'pipe': 117,\n",
       " 'big': 118,\n",
       " 'keep': 119,\n",
       " 'edt': 120,\n",
       " 'ever': 121,\n",
       " 'theres': 122,\n",
       " 'since': 123,\n",
       " 'lol': 124,\n",
       " 'god': 125,\n",
       " 'read': 126,\n",
       " 'another': 127,\n",
       " 'live': 128,\n",
       " 'river': 129,\n",
       " 'ecoli': 130,\n",
       " 'getting': 131,\n",
       " 'yes': 132,\n",
       " 'though': 133,\n",
       " 'ok': 134,\n",
       " 'says': 135,\n",
       " 'health': 136,\n",
       " 'next': 137,\n",
       " 'home': 138,\n",
       " 'country': 139,\n",
       " 'lot': 140,\n",
       " 'put': 141,\n",
       " 'state': 142,\n",
       " 'done': 143,\n",
       " 'money': 144,\n",
       " 'without': 145,\n",
       " 'e': 146,\n",
       " 'tell': 147,\n",
       " 'ill': 148,\n",
       " 'hope': 149,\n",
       " 'let': 150,\n",
       " 'nothing': 151,\n",
       " 'little': 152,\n",
       " 'give': 153,\n",
       " 'find': 154,\n",
       " 'everyone': 155,\n",
       " 'care': 156,\n",
       " 'thought': 157,\n",
       " 'actually': 158,\n",
       " 'week': 159,\n",
       " 'gonna': 160,\n",
       " 'flashflood': 161,\n",
       " 'used': 162,\n",
       " 'already': 163,\n",
       " 'issues': 164,\n",
       " 'anything': 165,\n",
       " 'toxic': 166,\n",
       " 'theyre': 167,\n",
       " 'long': 168,\n",
       " 'morning': 169,\n",
       " 'away': 170,\n",
       " 'public': 171,\n",
       " 'million': 172,\n",
       " 'person': 173,\n",
       " 'real': 174,\n",
       " 'show': 175,\n",
       " 'thank': 176,\n",
       " 'cause': 177,\n",
       " 'days': 178,\n",
       " 'areas': 179,\n",
       " 'call': 180,\n",
       " 'high': 181,\n",
       " 'continues': 182,\n",
       " 'alert': 183,\n",
       " 'change': 184,\n",
       " 'school': 185,\n",
       " 'part': 186,\n",
       " 'trying': 187,\n",
       " 'anyone': 188,\n",
       " 'safe': 189,\n",
       " 'old': 190,\n",
       " 'enough': 191,\n",
       " 'isnt': 192,\n",
       " 'heavy': 193,\n",
       " 'possible': 194,\n",
       " 'fuck': 195,\n",
       " 'point': 196,\n",
       " 'place': 197,\n",
       " 'thanks': 198,\n",
       " 'end': 199,\n",
       " 'free': 200,\n",
       " 'support': 201,\n",
       " 'white': 202,\n",
       " 'gt': 203,\n",
       " 'california': 204,\n",
       " 'happy': 205,\n",
       " 'maybe': 206,\n",
       " 'left': 207,\n",
       " 'system': 208,\n",
       " 'mean': 209,\n",
       " 'hard': 210,\n",
       " 'everything': 211,\n",
       " 'night': 212,\n",
       " 'must': 213,\n",
       " 'saying': 214,\n",
       " 'told': 215,\n",
       " 'start': 216,\n",
       " 'south': 217,\n",
       " 'found': 218,\n",
       " 'hate': 219,\n",
       " 'problem': 220,\n",
       " 'eu': 221,\n",
       " 'climate': 222,\n",
       " 'times': 223,\n",
       " 'near': 224,\n",
       " 'might': 225,\n",
       " 'wrong': 226,\n",
       " 'kids': 227,\n",
       " 'th': 228,\n",
       " 'looking': 229,\n",
       " 'children': 230,\n",
       " 'including': 231,\n",
       " 'full': 232,\n",
       " 'remember': 233,\n",
       " 'yet': 234,\n",
       " 'making': 235,\n",
       " 'believe': 236,\n",
       " 'story': 237,\n",
       " 'road': 238,\n",
       " 'w': 239,\n",
       " 'st': 240,\n",
       " 'across': 241,\n",
       " 'wont': 242,\n",
       " 'needs': 243,\n",
       " 'government': 244,\n",
       " 'coming': 245,\n",
       " 'team': 246,\n",
       " 'try': 247,\n",
       " 'makes': 248,\n",
       " 'family': 249,\n",
       " 'game': 250,\n",
       " 'house': 251,\n",
       " 'human': 252,\n",
       " 'job': 253,\n",
       " 'access': 254,\n",
       " 'learn': 255,\n",
       " 'beach': 256,\n",
       " 'friends': 257,\n",
       " 'co': 258,\n",
       " 'ago': 259,\n",
       " 'black': 260,\n",
       " 'talking': 261,\n",
       " 'check': 262,\n",
       " 'top': 263,\n",
       " 'n': 264,\n",
       " 'seen': 265,\n",
       " 'report': 266,\n",
       " 'least': 267,\n",
       " 'video': 268,\n",
       " 'went': 269,\n",
       " 'play': 270,\n",
       " 'national': 271,\n",
       " 'yeah': 272,\n",
       " 'stay': 273,\n",
       " 'environment': 274,\n",
       " 'bc': 275,\n",
       " 'service': 276,\n",
       " 'power': 277,\n",
       " 'post': 278,\n",
       " 'hours': 279,\n",
       " 'en': 280,\n",
       " 'different': 281,\n",
       " 'looks': 282,\n",
       " 'se': 283,\n",
       " 'understand': 284,\n",
       " 'pay': 285,\n",
       " 'far': 286,\n",
       " 'fucking': 287,\n",
       " 'hot': 288,\n",
       " 'pfas': 289,\n",
       " 'wait': 290,\n",
       " 'jul': 291,\n",
       " 'talk': 292,\n",
       " 'whats': 293,\n",
       " 'summer': 294,\n",
       " 'using': 295,\n",
       " 'issue': 296,\n",
       " 'working': 297,\n",
       " 'racist': 298,\n",
       " 'whole': 299,\n",
       " 'called': 300,\n",
       " 'guy': 301,\n",
       " 'ass': 302,\n",
       " 'tonight': 303,\n",
       " 'id': 304,\n",
       " 'literally': 305,\n",
       " 'chemical': 306,\n",
       " 'closed': 307,\n",
       " 'storm': 308,\n",
       " 'groundwater': 309,\n",
       " 'beaches': 310,\n",
       " 'counties': 311,\n",
       " 'india': 312,\n",
       " 'wasnt': 313,\n",
       " 'president': 314,\n",
       " 'chevron': 315,\n",
       " 'lets': 316,\n",
       " 'local': 317,\n",
       " 'tweet': 318,\n",
       " 'media': 319,\n",
       " 'hit': 320,\n",
       " 'sorry': 321,\n",
       " 'risk': 322,\n",
       " 'tuesday': 323,\n",
       " 'name': 324,\n",
       " 'guys': 325,\n",
       " 'saw': 326,\n",
       " 'came': 327,\n",
       " 'women': 328,\n",
       " 'supply': 329,\n",
       " 'gulf': 330,\n",
       " 'ask': 331,\n",
       " 'girl': 332,\n",
       " 'company': 333,\n",
       " 'warnings': 334,\n",
       " 'fact': 335,\n",
       " 'twitter': 336,\n",
       " 'else': 337,\n",
       " 'probably': 338,\n",
       " 'less': 339,\n",
       " 'soon': 340,\n",
       " 'reports': 341,\n",
       " 'drink': 342,\n",
       " 'reason': 343,\n",
       " 'na': 344,\n",
       " 'el': 345,\n",
       " 'pretty': 346,\n",
       " 'face': 347,\n",
       " 'hell': 348,\n",
       " 'started': 349,\n",
       " 'etc': 350,\n",
       " 'monday': 351,\n",
       " 'true': 352,\n",
       " 'others': 353,\n",
       " 'rainfall': 354,\n",
       " 'ms': 355,\n",
       " 'emergency': 356,\n",
       " 'close': 357,\n",
       " 'along': 358,\n",
       " 'coast': 359,\n",
       " 'north': 360,\n",
       " 'barry': 361,\n",
       " 'leave': 362,\n",
       " 'treatment': 363,\n",
       " 'gallons': 364,\n",
       " 'crisis': 365,\n",
       " 'business': 366,\n",
       " 'fun': 367,\n",
       " 'head': 368,\n",
       " 'open': 369,\n",
       " 'central': 370,\n",
       " 'nice': 371,\n",
       " 'matter': 372,\n",
       " 'kind': 373,\n",
       " 'gets': 374,\n",
       " 'friend': 375,\n",
       " 'sea': 376,\n",
       " 'run': 377,\n",
       " 'save': 378,\n",
       " 'update': 379,\n",
       " 'severe': 380,\n",
       " 'fire': 381,\n",
       " 'follow': 382,\n",
       " 'taking': 383,\n",
       " 'wants': 384,\n",
       " 'major': 385,\n",
       " 'important': 386,\n",
       " 'gas': 387,\n",
       " 'environmental': 388,\n",
       " 'car': 389,\n",
       " 'hear': 390,\n",
       " 'line': 391,\n",
       " 'affected': 392,\n",
       " 'able': 393,\n",
       " 'turn': 394,\n",
       " 'comes': 395,\n",
       " 'hour': 396,\n",
       " 'means': 397,\n",
       " 'bring': 398,\n",
       " 'wanted': 399,\n",
       " 'thinking': 400,\n",
       " 'happened': 401,\n",
       " 'response': 402,\n",
       " 'poor': 403,\n",
       " 'wouldnt': 404,\n",
       " 'almost': 405,\n",
       " 'idea': 406,\n",
       " 'heat': 407,\n",
       " 'ground': 408,\n",
       " 'running': 409,\n",
       " 'instead': 410,\n",
       " 'wanna': 411,\n",
       " 'goes': 412,\n",
       " 'bit': 413,\n",
       " 'dead': 414,\n",
       " 'k': 415,\n",
       " 'lost': 416,\n",
       " 'took': 417,\n",
       " 'parts': 418,\n",
       " 'small': 419,\n",
       " 'park': 420,\n",
       " 'living': 421,\n",
       " 'asked': 422,\n",
       " 'case': 423,\n",
       " 'damn': 424,\n",
       " 'mind': 425,\n",
       " 'massive': 426,\n",
       " 'community': 427,\n",
       " 'party': 428,\n",
       " 'seems': 429,\n",
       " 'energy': 430,\n",
       " 'amazing': 431,\n",
       " 'buy': 432,\n",
       " 'america': 433,\n",
       " 'die': 434,\n",
       " 'august': 435,\n",
       " 'caused': 436,\n",
       " 'red': 437,\n",
       " 'wish': 438,\n",
       " 'months': 439,\n",
       " 'southern': 440,\n",
       " 'phone': 441,\n",
       " 'past': 442,\n",
       " 'tomorrow': 443,\n",
       " 'following': 444,\n",
       " 'future': 445,\n",
       " 'deal': 446,\n",
       " 'together': 447,\n",
       " 'west': 448,\n",
       " 'guess': 449,\n",
       " 'air': 450,\n",
       " 'weeks': 451,\n",
       " 'hey': 452,\n",
       " 'oilspill': 453,\n",
       " 'land': 454,\n",
       " 'arent': 455,\n",
       " 'season': 456,\n",
       " 'plan': 457,\n",
       " 'da': 458,\n",
       " 'sad': 459,\n",
       " 'watermanagement': 460,\n",
       " 'heart': 461,\n",
       " 'gave': 462,\n",
       " 'either': 463,\n",
       " 'stuff': 464,\n",
       " 'plastic': 465,\n",
       " 'shows': 466,\n",
       " 'level': 467,\n",
       " 'huge': 468,\n",
       " 'body': 469,\n",
       " 'latest': 470,\n",
       " 'residents': 471,\n",
       " 'baby': 472,\n",
       " 'likely': 473,\n",
       " 'side': 474,\n",
       " 'eat': 475,\n",
       " 'green': 476,\n",
       " 'quality': 477,\n",
       " 'drive': 478,\n",
       " 'social': 479,\n",
       " 'list': 480,\n",
       " 'message': 481,\n",
       " 'control': 482,\n",
       " 'watching': 483,\n",
       " 'mom': 484,\n",
       " 'men': 485,\n",
       " 'gone': 486,\n",
       " 'states': 487,\n",
       " 'mississippi': 488,\n",
       " 'ready': 489,\n",
       " 'management': 490,\n",
       " 'law': 491,\n",
       " 'knew': 492,\n",
       " 'single': 493,\n",
       " 'ya': 494,\n",
       " 'dog': 495,\n",
       " 'win': 496,\n",
       " 'east': 497,\n",
       " 'wash': 498,\n",
       " 'move': 499,\n",
       " 'action': 500,\n",
       " 'levels': 501,\n",
       " 'woman': 502,\n",
       " 'playing': 503,\n",
       " 'ocean': 504,\n",
       " 'half': 505,\n",
       " 'late': 506,\n",
       " 'american': 507,\n",
       " 'island': 508,\n",
       " 'especially': 509,\n",
       " 'situation': 510,\n",
       " 'continue': 511,\n",
       " 'month': 512,\n",
       " 'climatechange': 513,\n",
       " 'question': 514,\n",
       " 'roads': 515,\n",
       " 'send': 516,\n",
       " 'yesterday': 517,\n",
       " 'group': 518,\n",
       " 'heard': 519,\n",
       " 'evening': 520,\n",
       " 'communities': 521,\n",
       " 'later': 522,\n",
       " 'services': 523,\n",
       " 'knows': 524,\n",
       " 'street': 525,\n",
       " 'aint': 526,\n",
       " 'b': 527,\n",
       " 'beautiful': 528,\n",
       " 'order': 529,\n",
       " 'become': 530,\n",
       " 'happen': 531,\n",
       " 'history': 532,\n",
       " 'ones': 533,\n",
       " 'kern': 534,\n",
       " 'worse': 535,\n",
       " 'police': 536,\n",
       " 'music': 537,\n",
       " 'fine': 538,\n",
       " 'spills': 539,\n",
       " 'damage': 540,\n",
       " 'havent': 541,\n",
       " 'main': 542,\n",
       " 'hi': 543,\n",
       " 'worst': 544,\n",
       " 'swimming': 545,\n",
       " 'fix': 546,\n",
       " 'minutes': 547,\n",
       " 'hand': 548,\n",
       " 'information': 549,\n",
       " 'miss': 550,\n",
       " 'al': 551,\n",
       " 'agree': 552,\n",
       " 'afternoon': 553,\n",
       " 'remains': 554,\n",
       " 'exactly': 555,\n",
       " 'site': 556,\n",
       " 'course': 557,\n",
       " 'chance': 558,\n",
       " 'un': 559,\n",
       " 'crazy': 560,\n",
       " 'act': 561,\n",
       " 'needed': 562,\n",
       " 'sunday': 563,\n",
       " 'provide': 564,\n",
       " 'closure': 565,\n",
       " 'rest': 566,\n",
       " 'office': 567,\n",
       " 'lmao': 568,\n",
       " 'early': 569,\n",
       " 'set': 570,\n",
       " 'per': 571,\n",
       " 'industry': 572,\n",
       " 'hair': 573,\n",
       " 'flooded': 574,\n",
       " 'bitch': 575,\n",
       " 'baltimore': 576,\n",
       " 'lives': 577,\n",
       " 'funny': 578,\n",
       " 'vote': 579,\n",
       " 'sir': 580,\n",
       " 'threat': 581,\n",
       " 'large': 582,\n",
       " 'alertgt': 583,\n",
       " 'protect': 584,\n",
       " 'project': 585,\n",
       " 'child': 586,\n",
       " 'event': 587,\n",
       " 'floods': 588,\n",
       " 'entire': 589,\n",
       " 'heres': 590,\n",
       " 'room': 591,\n",
       " 'fight': 592,\n",
       " 'biggest': 593,\n",
       " 'wednesday': 594,\n",
       " 'cool': 595,\n",
       " 'infrastructure': 596,\n",
       " 'three': 597,\n",
       " 'em': 598,\n",
       " 'safety': 599,\n",
       " 'clear': 600,\n",
       " 'listen': 601,\n",
       " 'shut': 602,\n",
       " 'book': 603,\n",
       " 'definitely': 604,\n",
       " 'ar': 605,\n",
       " 'sense': 606,\n",
       " 'break': 607,\n",
       " 'words': 608,\n",
       " 'storms': 609,\n",
       " 'behind': 610,\n",
       " 'girls': 611,\n",
       " 'seeing': 612,\n",
       " 'asking': 613,\n",
       " 'lack': 614,\n",
       " 'song': 615,\n",
       " 'son': 616,\n",
       " 'sick': 617,\n",
       " 'imagine': 618,\n",
       " 'spilled': 619,\n",
       " 'number': 620,\n",
       " 'mdt': 621,\n",
       " 'given': 622,\n",
       " 'stupid': 623,\n",
       " 'conditions': 624,\n",
       " 'wow': 625,\n",
       " 'systems': 626,\n",
       " 'share': 627,\n",
       " 'account': 628,\n",
       " 'nearly': 629,\n",
       " 'sleep': 630,\n",
       " 'sometimes': 631,\n",
       " 'second': 632,\n",
       " 'low': 633,\n",
       " 'movie': 634,\n",
       " 'source': 635,\n",
       " 'tried': 636,\n",
       " 'several': 637,\n",
       " 'include': 638,\n",
       " 'moment': 639,\n",
       " 'por': 640,\n",
       " 'visit': 641,\n",
       " 'hands': 642,\n",
       " 'x': 643,\n",
       " 'alone': 644,\n",
       " 'blue': 645,\n",
       " 'uk': 646,\n",
       " 'whatever': 647,\n",
       " 'wonder': 648,\n",
       " 'bill': 649,\n",
       " 'record': 650,\n",
       " 'plant': 651,\n",
       " 'giving': 652,\n",
       " 'mad': 653,\n",
       " 'building': 654,\n",
       " 'young': 655,\n",
       " 'study': 656,\n",
       " 'bottle': 657,\n",
       " 'basic': 658,\n",
       " 'taken': 659,\n",
       " 'serious': 660,\n",
       " 'rather': 661,\n",
       " 'meet': 662,\n",
       " 'article': 663,\n",
       " 'link': 664,\n",
       " 'global': 665,\n",
       " 'couldnt': 666,\n",
       " 'los': 667,\n",
       " 'cut': 668,\n",
       " 'research': 669,\n",
       " 'available': 670,\n",
       " 'waters': 671,\n",
       " 'cities': 672,\n",
       " 'town': 673,\n",
       " 'development': 674,\n",
       " 'fan': 675,\n",
       " 'hygiene': 676,\n",
       " 'rivers': 677,\n",
       " 'feeling': 678,\n",
       " 'natural': 679,\n",
       " 'avoid': 680,\n",
       " 'data': 681,\n",
       " 'within': 682,\n",
       " 'officials': 683,\n",
       " 'q': 684,\n",
       " 'expected': 685,\n",
       " 'lead': 686,\n",
       " 'largest': 687,\n",
       " 'experience': 688,\n",
       " 'boy': 689,\n",
       " 'rights': 690,\n",
       " 'c': 691,\n",
       " 'front': 692,\n",
       " 'truth': 693,\n",
       " 'problems': 694,\n",
       " 'idk': 695,\n",
       " 'waiting': 696,\n",
       " 'age': 697,\n",
       " 'valley': 698,\n",
       " 'outside': 699,\n",
       " 'attention': 700,\n",
       " 'cute': 701,\n",
       " 'contaminated': 702,\n",
       " 'pollution': 703,\n",
       " 'erie': 704,\n",
       " 'finally': 705,\n",
       " 'impact': 706,\n",
       " 'es': 707,\n",
       " 'harmful': 708,\n",
       " 'inches': 709,\n",
       " 'direct': 710,\n",
       " 'yo': 711,\n",
       " 'info': 712,\n",
       " 'kill': 713,\n",
       " 'works': 714,\n",
       " 'potential': 715,\n",
       " 'killed': 716,\n",
       " 'mine': 717,\n",
       " 'disaster': 718,\n",
       " 'stand': 719,\n",
       " 'excited': 720,\n",
       " 'moving': 721,\n",
       " 'daily': 722,\n",
       " 'com': 723,\n",
       " 'games': 724,\n",
       " 'reported': 725,\n",
       " 'dangerous': 726,\n",
       " 'thursday': 727,\n",
       " 'drop': 728,\n",
       " 'based': 729,\n",
       " 'deep': 730,\n",
       " 'sent': 731,\n",
       " 'worth': 732,\n",
       " 'youve': 733,\n",
       " 'weekend': 734,\n",
       " 'met': 735,\n",
       " 'para': 736,\n",
       " 'super': 737,\n",
       " 'military': 738,\n",
       " 'gun': 739,\n",
       " 'causing': 740,\n",
       " 'telling': 741,\n",
       " 'parents': 742,\n",
       " 'space': 743,\n",
       " 'tropical': 744,\n",
       " 'takes': 745,\n",
       " 'death': 746,\n",
       " 'portions': 747,\n",
       " 'plants': 748,\n",
       " 'word': 749,\n",
       " 'glad': 750,\n",
       " 'bro': 751,\n",
       " 'calling': 752,\n",
       " 'notice': 753,\n",
       " 'fish': 754,\n",
       " 'class': 755,\n",
       " 'war': 756,\n",
       " 'amount': 757,\n",
       " 'ca': 758,\n",
       " 'resources': 759,\n",
       " 'advisory': 760,\n",
       " 'pa': 761,\n",
       " 'often': 762,\n",
       " 'whos': 763,\n",
       " 'gotta': 764,\n",
       " 'join': 765,\n",
       " 'jobs': 766,\n",
       " 'fresh': 767,\n",
       " 'couple': 768,\n",
       " 'arkansas': 769,\n",
       " 'education': 770,\n",
       " 'department': 771,\n",
       " 'arwx': 772,\n",
       " 'plus': 773,\n",
       " 'happens': 774,\n",
       " 'lots': 775,\n",
       " 'security': 776,\n",
       " 'absolutely': 777,\n",
       " 'western': 778,\n",
       " 'eyes': 779,\n",
       " 'contact': 780,\n",
       " 'questions': 781,\n",
       " 'lie': 782,\n",
       " 'currently': 783,\n",
       " 'proud': 784,\n",
       " 'san': 785,\n",
       " 'fans': 786,\n",
       " 'speak': 787,\n",
       " 'stream': 788,\n",
       " 'forget': 789,\n",
       " 'northern': 790,\n",
       " 'earth': 791,\n",
       " 'inside': 792,\n",
       " 'honestly': 793,\n",
       " 'easy': 794,\n",
       " 'countries': 795,\n",
       " 'saturday': 796,\n",
       " 'expect': 797,\n",
       " 'light': 798,\n",
       " 'seem': 799,\n",
       " 'dry': 800,\n",
       " 'middle': 801,\n",
       " 'um': 802,\n",
       " 'tax': 803,\n",
       " 'political': 804,\n",
       " 'district': 805,\n",
       " 'television': 806,\n",
       " 'cannot': 807,\n",
       " 'pipeline': 808,\n",
       " 'bed': 809,\n",
       " 'add': 810,\n",
       " 'known': 811,\n",
       " 'current': 812,\n",
       " 'result': 813,\n",
       " 'canyon': 814,\n",
       " 'forward': 815,\n",
       " 'cost': 816,\n",
       " 'chennai': 817,\n",
       " 'recent': 818,\n",
       " 'l': 819,\n",
       " 'photo': 820,\n",
       " 'litres': 821,\n",
       " 'weve': 822,\n",
       " 'cold': 823,\n",
       " 'science': 824,\n",
       " 'shouldnt': 825,\n",
       " 'according': 826,\n",
       " 'rd': 827,\n",
       " 'favorite': 828,\n",
       " 'cleaning': 829,\n",
       " 'nation': 830,\n",
       " 'chemicals': 831,\n",
       " 'dude': 832,\n",
       " 'strong': 833,\n",
       " 'straight': 834,\n",
       " 'places': 835,\n",
       " 'normal': 836,\n",
       " 'type': 837,\n",
       " 'sa': 838,\n",
       " 'enjoy': 839,\n",
       " 'dream': 840,\n",
       " 'hold': 841,\n",
       " 'fake': 842,\n",
       " 'felt': 843,\n",
       " 'aug': 844,\n",
       " 'art': 845,\n",
       " 'private': 846,\n",
       " 'process': 847,\n",
       " 'lo': 848,\n",
       " 'looked': 849,\n",
       " 'companies': 850,\n",
       " 'africa': 851,\n",
       " 'democrats': 852,\n",
       " 'region': 853,\n",
       " 'fast': 854,\n",
       " 'mi': 855,\n",
       " 'americans': 856,\n",
       " 'seriously': 857,\n",
       " 'reading': 858,\n",
       " 'app': 859,\n",
       " 'turned': 860,\n",
       " 'self': 861,\n",
       " 'dad': 862,\n",
       " 'deserve': 863,\n",
       " 'fall': 864,\n",
       " 'short': 865,\n",
       " 'supposed': 866,\n",
       " 'con': 867,\n",
       " 'kid': 868,\n",
       " 'ice': 869,\n",
       " 'toilet': 870,\n",
       " 'sun': 871,\n",
       " 'sound': 872,\n",
       " 'quite': 873,\n",
       " 'eating': 874,\n",
       " 'locations': 875,\n",
       " 'bay': 876,\n",
       " 'millions': 877,\n",
       " 'diesel': 878,\n",
       " 'train': 879,\n",
       " 'common': 880,\n",
       " 'youll': 881,\n",
       " 'anymore': 882,\n",
       " 'te': 883,\n",
       " 'usa': 884,\n",
       " 'canada': 885,\n",
       " 'bluegreen': 886,\n",
       " 'mexico': 887,\n",
       " 'step': 888,\n",
       " 'obama': 889,\n",
       " 'changed': 890,\n",
       " 'farmers': 891,\n",
       " 'hurt': 892,\n",
       " 'market': 893,\n",
       " 'date': 894,\n",
       " 'nd': 895,\n",
       " 'le': 896,\n",
       " 'perfect': 897,\n",
       " 'thunderstorms': 898,\n",
       " 'details': 899,\n",
       " 'picture': 900,\n",
       " 'general': 901,\n",
       " 'p': 902,\n",
       " 'mother': 903,\n",
       " 'ways': 904,\n",
       " 'build': 905,\n",
       " 'awesome': 906,\n",
       " 'population': 907,\n",
       " 'lies': 908,\n",
       " 'tap': 909,\n",
       " 'meeting': 910,\n",
       " 'walk': 911,\n",
       " 'sign': 912,\n",
       " 'blame': 913,\n",
       " 'billion': 914,\n",
       " 'schools': 915,\n",
       " 'test': 916,\n",
       " 'trust': 917,\n",
       " 'paid': 918,\n",
       " 'sounds': 919,\n",
       " 'h': 920,\n",
       " 'create': 921,\n",
       " 'unless': 922,\n",
       " 'pool': 923,\n",
       " 'solution': 924,\n",
       " 'answer': 925,\n",
       " 'lt': 926,\n",
       " 'however': 927,\n",
       " 'store': 928,\n",
       " 'opinion': 929,\n",
       " 'online': 930,\n",
       " 'caught': 931,\n",
       " 'responsible': 932,\n",
       " 'govt': 933,\n",
       " 'higher': 934,\n",
       " 'ny': 935,\n",
       " 'updates': 936,\n",
       " 'platform': 937,\n",
       " 'address': 938,\n",
       " 'cleanup': 939,\n",
       " 'production': 940,\n",
       " 'birthday': 941,\n",
       " 'happening': 942,\n",
       " 'jesus': 943,\n",
       " 'recently': 944,\n",
       " 'towards': 945,\n",
       " 'technology': 946,\n",
       " 'longer': 947,\n",
       " 'citizens': 948,\n",
       " 'pra': 949,\n",
       " 'actual': 950,\n",
       " 'five': 951,\n",
       " 'none': 952,\n",
       " 'campaign': 953,\n",
       " 'di': 954,\n",
       " 'completely': 955,\n",
       " 'respect': 956,\n",
       " 'skin': 957,\n",
       " 'tweets': 958,\n",
       " 'starting': 959,\n",
       " 'weird': 960,\n",
       " 'training': 961,\n",
       " 'cover': 962,\n",
       " 'allowed': 963,\n",
       " 'patagonia': 964,\n",
       " 'nj': 965,\n",
       " 'nobody': 966,\n",
       " 'died': 967,\n",
       " 'missing': 968,\n",
       " 'friday': 969,\n",
       " 'meant': 970,\n",
       " 'loved': 971,\n",
       " 'raw': 972,\n",
       " 'florida': 973,\n",
       " 'joke': 974,\n",
       " 'posted': 975,\n",
       " 'four': 976,\n",
       " 'smart': 977,\n",
       " 'worked': 978,\n",
       " 'born': 979,\n",
       " 'trash': 980,\n",
       " 'pass': 981,\n",
       " 'interesting': 982,\n",
       " 'decades': 983,\n",
       " 'kinda': 984,\n",
       " 'force': 985,\n",
       " 'tired': 986,\n",
       " 'thousands': 987,\n",
       " 'earlier': 988,\n",
       " 'played': 989,\n",
       " 'medical': 990,\n",
       " 'despite': 991,\n",
       " 'mr': 992,\n",
       " 'form': 993,\n",
       " 'causes': 994,\n",
       " 'washington': 995,\n",
       " 'center': 996,\n",
       " 'stopped': 997,\n",
       " 'soil': 998,\n",
       " 'totally': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#word_to_id['<UNK>'] = 0\n",
    "print(type(word_to_id))\n",
    "word_to_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification</th>\n",
       "      <th>binary_class</th>\n",
       "      <th>text</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>token_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun, Sand And Sewage: Report Shows Many U.S. B...</td>\n",
       "      <td>[sun, sand, sewage, report, shows, many, us, b...</td>\n",
       "      <td>[871, 1794, 72, 266, 466, 64, 22, 310, 1505, 545]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Many U.S. Beaches Are Unsafe For Swimming, Rep...</td>\n",
       "      <td>[many, us, beaches, unsafe, swimming, report, ...</td>\n",
       "      <td>[64, 22, 310, 1505, 545, 266, 466]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Many U.S. Beaches Are Unsafe For Swimming, Rep...</td>\n",
       "      <td>[many, us, beaches, unsafe, swimming, report, ...</td>\n",
       "      <td>[64, 22, 310, 1505, 545, 266, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun, Sand And Sewage: Report Shows Many U.S. B...</td>\n",
       "      <td>[sun, sand, sewage, report, shows, many, us, b...</td>\n",
       "      <td>[871, 1794, 72, 266, 466, 64, 22, 310, 1505, 5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Thanks, EPA.\\n\\nSun, Sand And Sewage: Report S...</td>\n",
       "      <td>[thanks, epa, sun, sand, sewage, report, shows...</td>\n",
       "      <td>[198, 1201, 871, 1794, 72, 266, 466, 64, 22, 3...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  classification  binary_class  \\\n",
       "0       relevant             1   \n",
       "1       relevant             1   \n",
       "2       relevant             1   \n",
       "3       relevant             1   \n",
       "4       relevant             1   \n",
       "\n",
       "                                                text  \\\n",
       "0  Sun, Sand And Sewage: Report Shows Many U.S. B...   \n",
       "1  Many U.S. Beaches Are Unsafe For Swimming, Rep...   \n",
       "2  Many U.S. Beaches Are Unsafe For Swimming, Rep...   \n",
       "3  Sun, Sand And Sewage: Report Shows Many U.S. B...   \n",
       "4  Thanks, EPA.\\n\\nSun, Sand And Sewage: Report S...   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [sun, sand, sewage, report, shows, many, us, b...   \n",
       "1  [many, us, beaches, unsafe, swimming, report, ...   \n",
       "2  [many, us, beaches, unsafe, swimming, report, ...   \n",
       "3  [sun, sand, sewage, report, shows, many, us, b...   \n",
       "4  [thanks, epa, sun, sand, sewage, report, shows...   \n",
       "\n",
       "                                           token_int  \n",
       "0  [871, 1794, 72, 266, 466, 64, 22, 310, 1505, 545]  \n",
       "1                 [64, 22, 310, 1505, 545, 266, 466]  \n",
       "2                   [64, 22, 310, 1505, 545, 266, 0]  \n",
       "3  [871, 1794, 72, 266, 466, 64, 22, 310, 1505, 5...  \n",
       "4  [198, 1201, 871, 1794, 72, 266, 466, 64, 22, 3...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transform token by id\n",
    "def transform_to_ids(row):\n",
    "    return [word_to_id[w] if w in word_to_id else word_to_id['<UNK>'] for w in row]\n",
    "\n",
    "\n",
    "df['token_int'] = df['tokenized'].apply(lambda x: transform_to_ids(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the max_length\n",
    "max_length = 40\n",
    "\n",
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['token_int'].values, df['binary_class'].values,\n",
    "                                                   test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding our sequences\n",
    "X_train = pad_sequences(X_train, maxlen = max_length, value = 0)\n",
    "X_test = pad_sequences(X_test, maxlen = max_length, value = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` python\n",
    "# creating the embedding matrix\n",
    "embedding_index = {}\n",
    "\n",
    "trained_embeds = open('my_final_embeddings_tweets.npy', 'rb')\n",
    "for line in trained_embeds:\n",
    "    #if \n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "trained_embeds.close()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We now create a neural network that applies different filter sizes (2 and 3) \n",
    "# to word vectors, then concatenate the outputs and apply a classifier on top on that.\n",
    "# We make a multi-layered CNN with an embedding layer dimension of 128\n",
    "reset_graph()\n",
    "\n",
    "def multilayer_cnn():\n",
    "    graph_input = Input(shape = (25000, 128))\n",
    "    \n",
    "    convolutions = []\n",
    "    for filter_size in range(2, 4):\n",
    "        # We will have a number of 128 filters\n",
    "        # Could try 'Valid' padding\n",
    "        X = Convolution1D(128, filter_size, padding = 'same', activation = 'relu')(graph_input)\n",
    "        convolutions.append(X)\n",
    "        \n",
    "    graph_output = concatenate(convolutions, axis = 1)\n",
    "    graph_output = GlobalMaxPooling1D()(graph_output)\n",
    "    graph = Model(graph_input, graph_output)\n",
    "    \n",
    "    model = Sequential([Embedding(25000, 128, input_length = max_length),\n",
    "                    graph,\n",
    "                    Dropout(0.5),\n",
    "                    Dense(32, activation='relu'),\n",
    "                    Dense(2, activation='softmax')])\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer = Adam(), metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "\n",
    "estimator = KerasClassifier(build_fn = multilayer_cnn, epochs=25, batch_size = 1024, verbose = 0)\n",
    "kfold = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n",
    "#results = cross_val_score(estimator, X_train, y_train, cv = kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "results = cross_val_score(estimator, X_train, y_train, cv = kfold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9633300246338333, 0.0015915347069486215)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.mean(), results.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_2 = np_utils.to_categorical(y_train)\n",
    "#print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_2 = np_utils.to_categorical(y_test)\n",
    "#print(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50150 samples, validate on 12538 samples\n",
      "Epoch 1/10\n",
      "50150/50150 [==============================] - 42s 846us/step - loss: 0.4560 - acc: 0.7893 - val_loss: 0.1215 - val_acc: 0.9651\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.96507, saving model to trained_models/best_trained_model.hdf5\n",
      "Epoch 2/10\n",
      "50150/50150 [==============================] - 50s 995us/step - loss: 0.0952 - acc: 0.9723 - val_loss: 0.0916 - val_acc: 0.9729\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.96507 to 0.97288, saving model to trained_models/best_trained_model.hdf5\n",
      "Epoch 3/10\n",
      "50150/50150 [==============================] - 53s 1ms/step - loss: 0.0585 - acc: 0.9829 - val_loss: 0.0950 - val_acc: 0.9719\n",
      "\n",
      "Epoch 00003: val_acc did not improve from 0.97288\n",
      "Epoch 4/10\n",
      "50150/50150 [==============================] - 56s 1ms/step - loss: 0.0385 - acc: 0.9895 - val_loss: 0.1055 - val_acc: 0.9708\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.97288\n",
      "Epoch 5/10\n",
      "50150/50150 [==============================] - 51s 1ms/step - loss: 0.0278 - acc: 0.9925 - val_loss: 0.1210 - val_acc: 0.9666\n",
      "\n",
      "Epoch 00005: val_acc did not improve from 0.97288\n",
      "Epoch 6/10\n",
      "50150/50150 [==============================] - 55s 1ms/step - loss: 0.0209 - acc: 0.9940 - val_loss: 0.1321 - val_acc: 0.9682\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.97288\n",
      "Epoch 7/10\n",
      "50150/50150 [==============================] - 49s 985us/step - loss: 0.0162 - acc: 0.9955 - val_loss: 0.1479 - val_acc: 0.9687\n",
      "\n",
      "Epoch 00007: val_acc did not improve from 0.97288\n",
      "Epoch 8/10\n",
      "50150/50150 [==============================] - 55s 1ms/step - loss: 0.0121 - acc: 0.9965 - val_loss: 0.1593 - val_acc: 0.9679\n",
      "\n",
      "Epoch 00008: val_acc did not improve from 0.97288\n",
      "Epoch 9/10\n",
      "50150/50150 [==============================] - 50s 993us/step - loss: 0.0108 - acc: 0.9966 - val_loss: 0.1686 - val_acc: 0.9663\n",
      "\n",
      "Epoch 00009: val_acc did not improve from 0.97288\n",
      "Epoch 10/10\n",
      "50150/50150 [==============================] - 53s 1ms/step - loss: 0.0095 - acc: 0.9973 - val_loss: 0.1780 - val_acc: 0.9672\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.97288\n"
     ]
    }
   ],
   "source": [
    "# Add checkpoint and use most accurate model from the epochs\n",
    "# Best model based on maximum accuracy\n",
    "best_model = ModelCheckpoint('trained_models/best_trained_model.hdf5', monitor = 'val_acc', verbose = 1,\n",
    "                             save_best_only = True, mode = 'max')\n",
    "model_to_use = [best_model]\n",
    "\n",
    "model = multilayer_cnn()\n",
    "model.fit(X_train, y_train_2, validation_data = (X_test, y_test_2), epochs = 10, batch_size = 1024, callbacks = model_to_use)\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9672196522571384\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.97      7191\n",
      "           1       0.97      0.96      0.96      5347\n",
      "\n",
      "    accuracy                           0.97     12538\n",
      "   macro avg       0.97      0.97      0.97     12538\n",
      "weighted avg       0.97      0.97      0.97     12538\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score(y_test, np.argmax(predictions, axis=1)))\n",
    "print(metrics.classification_report(y_test, np.argmax(predictions, axis=1)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
