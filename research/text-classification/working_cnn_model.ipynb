{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Flatten, Dense, Dropout, Convolution1D, MaxPooling1D, SpatialDropout1D, Input \n",
    "from keras.layers import GlobalMaxPooling1D, concatenate, LSTM, Bidirectional\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import scipy.stats as stats\n",
    "import string\n",
    "import re\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaner found on github: https://github.com/martinpella/twitter-airlines/blob/master/utils.py\n",
    "class TextCleaner(BaseEstimator, TransformerMixin):    \n",
    "    def remove_mentions(self, text):        \n",
    "        return re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    def remove_urls(self, text):        \n",
    "        return re.sub(r'http.?://[^\\s]+[\\s]?', '', text)\n",
    "    \n",
    "    def only_characters(self, text):\n",
    "        return re.sub('[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    def remove_extra_spaces(self, text):\n",
    "        text = re.sub(\"\\s+\", ' ', text)\n",
    "        text = text.lstrip()\n",
    "        return text.rstrip()\n",
    "    \n",
    "    def to_lower(self, text):\n",
    "        return text.lower()\n",
    "    \n",
    "    def fix_words(self, text):\n",
    "        text = re.sub(r'\\bthx\\b', 'thanks', text)\n",
    "        text = re.sub(r'\\bu\\b', 'you', text)\n",
    "        text = re.sub(r'\\bhrs\\b', 'hours', text)\n",
    "        text = re.sub(r'\\baa\\b', 'a', text)\n",
    "        text = re.sub(r'\\bflightr\\b', 'flight', text)\n",
    "        text = re.sub(r'\\bur\\b', 'your', text)\n",
    "        text = re.sub(r'\\bhr\\b', 'hour', text)\n",
    "        text = re.sub(r'\\bthru\\b', 'through', text)\n",
    "        text = re.sub(r'\\br\\b', 'are', text)\n",
    "        text = re.sub(r'\\bppl\\b', 'people', text)\n",
    "        text = re.sub(r'\\btix\\b', 'fix', text)\n",
    "        text = re.sub(r'\\bplz\\b', 'please', text)\n",
    "        text = re.sub(r'\\bflightd\\b', 'flighted', text)\n",
    "        text = re.sub(r'\\btmrw\\b', 'tomorrow', text)\n",
    "        text = re.sub(r'\\bthx\\b', 'thanks', text)\n",
    "        text = re.sub(r'\\bpls\\b', 'please', text)\n",
    "        text = re.sub(r'\\bfyi\\b', 'for your information', text)\n",
    "        \n",
    "        text = re.sub(r'\\bheyyyy\\b', 'hey', text)\n",
    "        text = re.sub(r'\\bguyyyys\\b', 'guys', text)\n",
    "        text = re.sub(r'\\byall\\b', 'you all', text)\n",
    "        text = re.sub(r'\\basap\\b', 'as soon as possible', text)\n",
    "        text = re.sub(r'\\bbtw\\b', 'by the way', text)\n",
    "        text = re.sub(r'\\bdm\\b', 'direct message', text)\n",
    "        text = re.sub(r'\\bcudtomers\\b', 'customers', text)\n",
    "        text = re.sub(r'\\bwtf\\b', 'what the fuck', text)\n",
    "        text = re.sub(r'\\biphone\\b', 'phone', text)\n",
    "        text = re.sub(r'\\bmins\\b', 'minutes', text)\n",
    "        text = re.sub(r'\\btv\\b', 'television', text)\n",
    "        text = re.sub(r'\\bokay\\b', 'ok', text)\n",
    "        text = re.sub(r'\\bfeb\\b', 'february', text)\n",
    "        text = re.sub(r'\\byr\\b', 'year', text)\n",
    "        text = re.sub(r'\\bshes\\b', 'she is', text)\n",
    "        text = re.sub(r'\\bnope\\b', 'no', text)\n",
    "        text = re.sub(r'\\bhes\\b', 'he is', text)\n",
    "        text = re.sub(r'\\btill\\b', 'until', text)\n",
    "        text = re.sub(r'\\bomg\\b', 'oh my god', text)\n",
    "        text = re.sub(r'\\btho\\b', 'though', text)\n",
    "        text = re.sub(r'\\bnothappy\\b', 'not happy', text)\n",
    "        return re.sub(r'\\bthankyou\\b', 'thank you', text)\n",
    "        \n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, **transform_params):        \n",
    "        clean_X = X.apply(self.remove_mentions).apply(self.remove_urls).apply(self.only_characters).apply(self.remove_extra_spaces).apply(self.to_lower).apply(self.fix_words)\n",
    "        return clean_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification</th>\n",
       "      <th>binary_class</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun, Sand And Sewage: Report Shows Many U.S. B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Many U.S. Beaches Are Unsafe For Swimming, Rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Many U.S. Beaches Are Unsafe For Swimming, Rep...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun, Sand And Sewage: Report Shows Many U.S. B...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun, Sand And Sewage: Report Shows Many U.S. B...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  classification  binary_class  \\\n",
       "0       relevant             1   \n",
       "1       relevant             1   \n",
       "2       relevant             1   \n",
       "3       relevant             1   \n",
       "4       relevant             1   \n",
       "\n",
       "                                                text  \n",
       "0  Sun, Sand And Sewage: Report Shows Many U.S. B...  \n",
       "1  Many U.S. Beaches Are Unsafe For Swimming, Rep...  \n",
       "2  Many U.S. Beaches Are Unsafe For Swimming, Rep...  \n",
       "3  Sun, Sand And Sewage: Report Shows Many U.S. B...  \n",
       "4  Sun, Sand And Sewage: Report Shows Many U.S. B...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the data\n",
    "relevant_tweets = pd.read_hdf('datasets/relevant_tweets.h5', 'relevant_tweets')\n",
    "relevant_tweets['classification'] = 'relevant'\n",
    "relevant_tweets['binary_class'] = np.ones(len(relevant_tweets)).astype(int)\n",
    "relevant_tweets = relevant_tweets[['classification', 'binary_class', 'text']]\n",
    "irrelevant_tweets = pd.read_hdf('datasets/not_relevant_tweets.h5', 'not_relevant_tweets')\n",
    "irrelevant_tweets['classification'] = 'irrelevant'\n",
    "irrelevant_tweets['binary_class'] = np.zeros(len(irrelevant_tweets)).astype(int)\n",
    "irrelevant_tweets = irrelevant_tweets[['classification', 'binary_class', 'text']]\n",
    "df = pd.concat([relevant_tweets, irrelevant_tweets]).reset_index()\n",
    "df = df.iloc[:, 1:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification</th>\n",
       "      <th>binary_class</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun, Sand And Sewage: Report Shows Many U.S. B...</td>\n",
       "      <td>sun sand and sewage report shows many us beach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Many U.S. Beaches Are Unsafe For Swimming, Rep...</td>\n",
       "      <td>many us beaches are unsafe for swimming report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Many U.S. Beaches Are Unsafe For Swimming, Rep...</td>\n",
       "      <td>many us beaches are unsafe for swimming report...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun, Sand And Sewage: Report Shows Many U.S. B...</td>\n",
       "      <td>sun sand and sewage report shows many us beach...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun, Sand And Sewage: Report Shows Many U.S. B...</td>\n",
       "      <td>sun sand and sewage report shows many us beach...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  classification  binary_class  \\\n",
       "0       relevant             1   \n",
       "1       relevant             1   \n",
       "2       relevant             1   \n",
       "3       relevant             1   \n",
       "4       relevant             1   \n",
       "\n",
       "                                                text  \\\n",
       "0  Sun, Sand And Sewage: Report Shows Many U.S. B...   \n",
       "1  Many U.S. Beaches Are Unsafe For Swimming, Rep...   \n",
       "2  Many U.S. Beaches Are Unsafe For Swimming, Rep...   \n",
       "3  Sun, Sand And Sewage: Report Shows Many U.S. B...   \n",
       "4  Sun, Sand And Sewage: Report Shows Many U.S. B...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  sun sand and sewage report shows many us beach...  \n",
       "1  many us beaches are unsafe for swimming report...  \n",
       "2  many us beaches are unsafe for swimming report...  \n",
       "3  sun sand and sewage report shows many us beach...  \n",
       "4  sun sand and sewage report shows many us beach...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean the text\n",
    "tc = TextCleaner()\n",
    "df['cleaned_text'] = tc.transform(df['text'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification</th>\n",
       "      <th>binary_class</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun, Sand And Sewage: Report Shows Many U.S. B...</td>\n",
       "      <td>sun sand and sewage report shows many us beach...</td>\n",
       "      <td>[sun, sand, and, sewage, report, shows, many, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Many U.S. Beaches Are Unsafe For Swimming, Rep...</td>\n",
       "      <td>many us beaches are unsafe for swimming report...</td>\n",
       "      <td>[many, us, beaches, are, unsafe, for, swimming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Many U.S. Beaches Are Unsafe For Swimming, Rep...</td>\n",
       "      <td>many us beaches are unsafe for swimming report...</td>\n",
       "      <td>[many, us, beaches, are, unsafe, for, swimming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun, Sand And Sewage: Report Shows Many U.S. B...</td>\n",
       "      <td>sun sand and sewage report shows many us beach...</td>\n",
       "      <td>[sun, sand, and, sewage, report, shows, many, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun, Sand And Sewage: Report Shows Many U.S. B...</td>\n",
       "      <td>sun sand and sewage report shows many us beach...</td>\n",
       "      <td>[sun, sand, and, sewage, report, shows, many, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  classification  binary_class  \\\n",
       "0       relevant             1   \n",
       "1       relevant             1   \n",
       "2       relevant             1   \n",
       "3       relevant             1   \n",
       "4       relevant             1   \n",
       "\n",
       "                                                text  \\\n",
       "0  Sun, Sand And Sewage: Report Shows Many U.S. B...   \n",
       "1  Many U.S. Beaches Are Unsafe For Swimming, Rep...   \n",
       "2  Many U.S. Beaches Are Unsafe For Swimming, Rep...   \n",
       "3  Sun, Sand And Sewage: Report Shows Many U.S. B...   \n",
       "4  Sun, Sand And Sewage: Report Shows Many U.S. B...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  sun sand and sewage report shows many us beach...   \n",
       "1  many us beaches are unsafe for swimming report...   \n",
       "2  many us beaches are unsafe for swimming report...   \n",
       "3  sun sand and sewage report shows many us beach...   \n",
       "4  sun sand and sewage report shows many us beach...   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [sun, sand, and, sewage, report, shows, many, ...  \n",
       "1  [many, us, beaches, are, unsafe, for, swimming...  \n",
       "2  [many, us, beaches, are, unsafe, for, swimming...  \n",
       "3  [sun, sand, and, sewage, report, shows, many, ...  \n",
       "4  [sun, sand, and, sewage, report, shows, many, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "\n",
    "def tokenize(s): \n",
    "    return re_tok.sub(r' \\1 ', s).split()\n",
    "\n",
    "df['tokenized'] = df['cleaned_text'].apply(lambda row: tokenize(row))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62688\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>classification</th>\n",
       "      <th>binary_class</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun, Sand And Sewage: Report Shows Many U.S. B...</td>\n",
       "      <td>sun sand and sewage report shows many us beach...</td>\n",
       "      <td>[sun, sand, and, sewage, report, shows, many, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Many U.S. Beaches Are Unsafe For Swimming, Rep...</td>\n",
       "      <td>many us beaches are unsafe for swimming report...</td>\n",
       "      <td>[many, us, beaches, are, unsafe, for, swimming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Many U.S. Beaches Are Unsafe For Swimming, Rep...</td>\n",
       "      <td>many us beaches are unsafe for swimming report...</td>\n",
       "      <td>[many, us, beaches, are, unsafe, for, swimming...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Sun, Sand And Sewage: Report Shows Many U.S. B...</td>\n",
       "      <td>sun sand and sewage report shows many us beach...</td>\n",
       "      <td>[sun, sand, and, sewage, report, shows, many, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>relevant</td>\n",
       "      <td>1</td>\n",
       "      <td>Thanks, EPA.\\n\\nSun, Sand And Sewage: Report S...</td>\n",
       "      <td>thanks epa sun sand and sewage report shows ma...</td>\n",
       "      <td>[thanks, epa, sun, sand, and, sewage, report, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  classification  binary_class  \\\n",
       "0       relevant             1   \n",
       "1       relevant             1   \n",
       "2       relevant             1   \n",
       "3       relevant             1   \n",
       "4       relevant             1   \n",
       "\n",
       "                                                text  \\\n",
       "0  Sun, Sand And Sewage: Report Shows Many U.S. B...   \n",
       "1  Many U.S. Beaches Are Unsafe For Swimming, Rep...   \n",
       "2  Many U.S. Beaches Are Unsafe For Swimming, Rep...   \n",
       "3  Sun, Sand And Sewage: Report Shows Many U.S. B...   \n",
       "4  Thanks, EPA.\\n\\nSun, Sand And Sewage: Report S...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  sun sand and sewage report shows many us beach...   \n",
       "1  many us beaches are unsafe for swimming report...   \n",
       "2  many us beaches are unsafe for swimming report...   \n",
       "3  sun sand and sewage report shows many us beach...   \n",
       "4  thanks epa sun sand and sewage report shows ma...   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [sun, sand, and, sewage, report, shows, many, ...  \n",
       "1  [many, us, beaches, are, unsafe, for, swimming...  \n",
       "2  [many, us, beaches, are, unsafe, for, swimming...  \n",
       "3  [sun, sand, and, sewage, report, shows, many, ...  \n",
       "4  [thanks, epa, sun, sand, and, sewage, report, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates(subset = ['cleaned_text']).reset_index()\n",
    "df = df.iloc[:, 1:]\n",
    "print(len(df))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "stop = set(stopwords.words('english'))\n",
    "stop.update(['amp', 'rt', 'cc'])\n",
    "stop = stop - set(['no', 'not'])\n",
    "\n",
    "def remove_stopwords(row):\n",
    "    return [t for t in row if t not in stop]\n",
    "\n",
    "df['tokenized'] = df['tokenized'].apply(lambda row: remove_stopwords(row))\n",
    "\n",
    "df = df[['classification', 'binary_class', 'text', 'tokenized']]\n",
    "vocab_counter = collections.Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sun', 'sand']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = np.array(df['tokenized']).reshape(-1, 1)\n",
    "words = []\n",
    "for i in range(len(tokens)):\n",
    "    tweet = tokens[i]\n",
    "    for j in range(len(tweet)):\n",
    "        new_word = tweet[j]\n",
    "        words.append(new_word)\n",
    "\n",
    "print(type(words))\n",
    "indivs = []\n",
    "for tweet in range(len(words)):\n",
    "    t = words[tweet]\n",
    "    for j in range(len(t)):\n",
    "        indivs.append(t[j])\n",
    "indivs[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40002\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 50000\n",
    "from collections import Counter\n",
    "\n",
    "# UNK = unknown words, HST = hashtag, EMT = emoticon, URL is self-explanatory, THDL = Twitter handle\n",
    "vocabulary = [(\"<UNK>\", None)] + [(\"<THDL>\", None)] +\\\n",
    "            Counter(indivs).most_common(vocabulary_size - 1)\n",
    "vocabulary = np.array([word for word, _ in vocabulary])\n",
    "dictionary = {word: code for code, word in enumerate(vocabulary)}\n",
    "data = np.array([dictionary.get(word, 0) for word in indivs])\n",
    "print(len(vocabulary))\n",
    "#print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 0 13.877185426237876\n"
     ]
    }
   ],
   "source": [
    "lengths = df['tokenized'].apply(lambda x: len(x))\n",
    "print(max(lengths), min(lengths), np.mean(lengths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'token_int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2656\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2657\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'token_int'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-4fd626a1d3fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Split the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m X_train, X_test, y_train, y_test = train_test_split(df['token_int'].values, df['binary_class'].values,\n\u001b[0m\u001b[0;32m      7\u001b[0m                                                    test_size = 0.2, random_state = 42)\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2925\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2926\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2927\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2928\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2929\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2657\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2658\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2659\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2661\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'token_int'"
     ]
    }
   ],
   "source": [
    "# set the max_length\n",
    "max_length = 50\n",
    "\n",
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['tokenized'].values, df['binary_class'].values,\n",
    "                                                   test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# padding our sequences\n",
    "X_train = pad_sequences(X_train, maxlen = max_length, value = -1)\n",
    "X_test = pad_sequences(X_test, maxlen = max_length, value = -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the embedding matrix\n",
    "def embed_matrix(vocab_size, embedding_dimension):\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dimension))\n",
    "    found = 0\n",
    "    for word, i in word_to_id.items():\n",
    "        embedding_vector = .get(word)\n",
    "        if i < vocab_size:\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                found += 1\n",
    "            else:\n",
    "                embedding_matrix[i] = np.random.normal(scale=0.6, size=(embedding_dim,))\n",
    "    return embedding_matrix, found\n",
    "\n",
    "embedding_matrix, found = create_emb_matrix(vocab_size, 128)\n",
    "\n",
    "print(found)\n",
    "\n",
    "# take different filter sizes to word vectors, then concatenate the outputs and apply a classifier on top of that\n",
    "def multilayer_cnn_model():\n",
    "    graph_input = Input(shape = (vocab_size, ))\n",
    "    \n",
    "    model = Sequential([Embedding(input_dim = vocab_size, output_dim = 128, input_len = max_length),\n",
    "        Convolution1D()\n",
    "    ])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
